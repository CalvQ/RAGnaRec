{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olive Garden ORG\n",
      "Halloween DATE\n",
      "Las Vegas GPE\n",
      "Frankfurt GPE\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "text = \"I love the pasta at Olive Garden, especially on Halloween. Las Vegas, Frankfurt\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvq/miniconda3/envs/10701-env/lib/python3.10/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL: Numerals that do not fall under another type\n",
      "DATE: Absolute or relative dates or periods\n",
      "EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
      "FAC: Buildings, airports, highways, bridges, etc.\n",
      "GPE: Countries, cities, states\n",
      "LANGUAGE: Any named language\n",
      "LAW: Named documents made into laws.\n",
      "LOC: Non-GPE locations, mountain ranges, bodies of water\n",
      "MONEY: Monetary values, including unit\n",
      "NORP: Nationalities or religious or political groups\n",
      "ORDINAL: \"first\", \"second\", etc.\n",
      "ORG: Companies, agencies, institutions, etc.\n",
      "PERCENT: Percentage, including \"%\"\n",
      "PERSON: People, including fictional\n",
      "PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
      "QUANTITY: Measurements, as of weight or distance\n",
      "TIME: Times smaller than a day\n",
      "WORK_OF_ART: Titles of books, songs, etc.\n"
     ]
    }
   ],
   "source": [
    "# View all spacy labels\n",
    "nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "for label in nlp.get_pipe('ner').labels:\n",
    "    print(f\"{label}: {spacy.explain(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love the pasta at <ORG>, especially on <DATE>.\n"
     ]
    }
   ],
   "source": [
    "def replace_specific_entities(doc, target_labels):\n",
    "    replaced_text = doc.text\n",
    "    for ent in sorted(doc.ents, key=lambda e: e.start_char, reverse=True):\n",
    "        if ent.label_ in target_labels:\n",
    "            token = f\"<{ent.label_}>\"\n",
    "            replaced_text = replaced_text[:ent.start_char] + token + replaced_text[ent.end_char:]\n",
    "    return replaced_text\n",
    "\n",
    "# Specify the labels to target\n",
    "result = replace_specific_entities(doc, target_labels=[\"FAC\", \"ORG\", \"DATE\"])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet', 'test': 'yelp_review_full/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/Yelp/yelp_review_full/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df.head(1000)[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvq/miniconda3/envs/10701-env/lib/python3.10/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "TARGET_LABELS = [\"FAC\", \"ORG\", \"PERSON\"]\n",
    "REPLACEMENT_LABELS = [\"FAC\", \"ORG\", \"PERSON\"]\n",
    "\n",
    "tag_list = \"|\".join([label.lower() for label in REPLACEMENT_LABELS])\n",
    "token_regex = fr\"(?:\\b\\w+\\b|<(?:{tag_list})>)\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(?:\\\\b\\\\w+\\\\b|<(?:fac|org|person)>)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n",
      "goldberg PERSON\n",
      "nyu ORG\n",
      "first ORDINAL\n"
     ]
    }
   ],
   "source": [
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    containFlag = False\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text, ent.label_)\n",
    "        if ent.label_ in TARGET_LABELS:\n",
    "            containFlag = True\n",
    "    if containFlag:\n",
    "        print(review)\n",
    "        for ent in doc.ents:\n",
    "            print(ent.text, ent.label_)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole Foods ORG\n",
      "the Whole Foods ORG\n",
      "Las Vegas GPE\n"
     ]
    }
   ],
   "source": [
    "text = \"Compared to other Whole Foods locations, this one is tiny! For comparison, the Whole Foods in Las Vegas\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running spaCy NER on Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet', 'test': 'yelp_review_full/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/Yelp/yelp_review_full/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df.head(1000)[\"text\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #removes 's and apostrophe, converts to lower case\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"'s(\\s|$)\", r\"\\1\", text)\n",
    "    text = re.sub(r\"'\", r\"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "tokenizer = RegexpTokenizer(token_regex)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "english_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<person>\n",
      "<person>s\n",
      "la\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"<person>\"))\n",
    "print(lemmatizer.lemmatize(\"<person>s\"))\n",
    "print(lemmatizer.lemmatize(\"las\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_LABELS = [\"FAC\", \"ORG\", \"PERSON\", \"PRODUCT\", \"WORK_OF_ART\"]\n",
    "TARGET_LABELS = [\"FAC\", \"ORG\", \"PERSON\"]\n",
    "REPLACEMENT_LABELS = [\"FAC\", \"ORG\", \"PERSON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvq/miniconda3/envs/10701-env/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "# 3m 41.9s\n",
    "filtered_reviews = []\n",
    "\n",
    "for idx, review in enumerate(reviews):\n",
    "    doc = nlp(review)\n",
    "    contain_flag = False\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in TARGET_LABELS:\n",
    "            contain_flag = True\n",
    "    if contain_flag:\n",
    "        # print(review)\n",
    "        result = replace_specific_entities(doc, target_labels=REPLACEMENT_LABELS)\n",
    "        filtered_reviews.append((result, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/calvq/miniconda3/envs/10701-env/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared to other <ORG> locations, this one is tiny! For comparison, <ORG> in Las Vegas must be at least three times the size. Still, it's nice to have and it is pretty decent overall albeit pricey.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Compared to other Whole Foods locations, this one is tiny! For comparison, the Whole Foods in Las Vegas must be at least three times the size. Still, it's nice to have and it is pretty decent overall albeit pricey.\")\n",
    "print(replace_specific_entities(doc, target_labels=REPLACEMENT_LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"dr. <PERSON> offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (<ORG>) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\",\n",
       " 0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_reviews)\n",
    "filtered_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    output = []\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag([token])[0][1]\n",
    "        lemma = lemmatizer.lemmatize(token, pos=get_wordnet_pos(pos_tag))\n",
    "        if lemma not in english_stopwords and len(lemma) >= 2:\n",
    "            output.append(lemma)\n",
    "    return output\n",
    "    \n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s<>/]\", \"\", text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    cleaned_tokens = clean_tokens(tokens)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', '<person>', 'offers', 'everything', 'i', 'look', 'for', 'in', 'a', 'general', 'practitioner', 'hes', 'nice', 'and', 'easy', 'to', 'talk', 'to', 'without', 'being', 'patronizing', 'hes', 'always', 'on', 'time', 'in', 'seeing', 'his', 'patients', 'hes', 'affiliated', 'with', 'a', 'topnotch', 'hospital', '<org>', 'which', 'my', 'parents', 'have', 'explained', 'to', 'me', 'is', 'very', 'important', 'in', 'case', 'something', 'happens', 'and', 'you', 'need', 'surgery', 'and', 'you', 'can', 'get', 'referrals', 'to', 'see', 'specialists', 'without', 'having', 'to', 'see', 'him', 'first', 'really', 'what', 'more', 'do', 'you', 'need', 'im', 'sitting', 'here', 'trying', 'to', 'think', 'of', 'any', 'complaints', 'i', 'have', 'about', 'him', 'but', 'im', 'really', 'drawing', 'a', 'blank']\n",
      "['unfortunately', 'the', 'frustration', 'of', 'being', 'dr', '<person>', 's', 'patient', 'is', 'a', 'repeat', 'of', 'the', 'experience', 'ive', 'had', 'with', 'so', 'many', 'other', 'doctors', 'in', 'nyc', 'good', 'doctor', 'terrible', 'staff', 'it', 'seems', 'that', 'his', 'staff', 'simply', 'never', 'answers', 'the', 'phone', 'it', 'usually', 'takes', '2', 'hours', 'of', 'repeated', 'calling', 'to', 'get', 'an', 'answer', 'who', 'has', 'time', 'for', 'that', 'or', 'wants', 'to', 'deal', 'with', 'it', 'i', 'have', 'run', 'into', 'this', 'problem', 'with', 'many', 'other', 'doctors', 'and', 'i', 'just', 'dont', 'get', 'it', 'you', 'have', 'office', 'workers', 'you', 'have', 'patients', 'with', 'medical', 'needs', 'why', 'isnt', 'anyone', 'answering', 'the', 'phone', 'its', 'incomprehensible', 'and', 'not', 'work', 'the', 'aggravation', 'its', 'with', 'regret', 'that', 'i', 'feel', 'that', 'i', 'have', 'to', 'give', 'dr', '<person>', '2', 'stars']\n",
      "['been', 'going', 'to', 'dr', '<person>', 'for', 'over', '10', 'years', 'i', 'think', 'i', 'was', 'one', 'of', 'his', '1st', 'patients', 'when', 'he', 'started', 'at', '<org>', 'hes', 'been', 'great', 'over', 'the', 'years', 'and', 'is', 'really', 'all', 'about', 'the', 'big', 'picture', 'it', 'is', 'because', 'of', 'him', 'not', 'my', 'now', 'former', 'gyn', 'dr', '<person>', 'that', 'i', 'found', 'out', 'i', 'have', 'fibroids', 'he', 'explores', 'all', 'options', 'with', 'you', 'and', 'is', 'very', 'patient', 'and', 'understanding', 'he', 'doesnt', 'judge', 'and', 'asks', 'all', 'the', 'right', 'questions', 'very', 'thorough', 'and', 'wants', 'to', 'be', 'kept', 'in', 'the', 'loop', 'on', 'every', 'aspect', 'of', 'your', 'medical', 'health', 'and', 'your', 'life']\n",
      "['got', 'a', 'letter', 'in', 'the', 'mail', 'last', 'week', 'that', 'said', 'dr', '<person>', 'is', 'moving', 'to', 'arizona', 'to', 'take', 'a', 'new', 'position', 'there', 'in', 'june', 'he', 'will', 'be', 'missed', 'very', 'much', 'nni', 'think', 'finding', 'a', 'new', 'doctor', 'in', 'nyc', 'that', 'you', 'actually', 'like', 'might', 'almost', 'be', 'as', 'awful', 'as', 'trying', 'to', 'find', 'a', 'date']\n",
      "['i', 'dont', 'know', 'what', 'dr', '<person>', 'was', 'like', 'before', 'moving', 'to', 'arizona', 'but', 'let', 'me', 'tell', 'you', 'stay', 'away', 'from', 'this', 'doctor', 'and', 'this', 'office', 'i', 'was', 'going', 'to', 'dr', '<person>', 'before', 'he', 'left', 'and', '<person>', 'took', 'over', 'when', '<person>', 'left', 'he', 'is', 'not', 'a', 'caring', 'doctor', 'he', 'is', 'only', 'interested', 'in', 'the', 'copay', 'and', 'having', 'you', 'come', 'in', 'for', 'medication', 'refills', 'every', 'month', 'he', 'will', 'not', 'give', 'refills', 'and', 'could', 'less', 'about', 'patientss', 'financial', 'situations', 'trying', 'to', 'get', 'your', '90', 'days', 'mail', 'away', 'pharmacy', 'prescriptions', 'through', 'this', 'guy', 'is', 'a', 'joke', 'and', 'to', 'make', 'matters', 'even', 'worse', 'his', 'office', 'staff', 'is', 'incompetent', '90', 'of', 'the', 'time', 'when', 'you', 'call', 'the', 'office', 'theyll', 'put', 'you', 'through', 'to', 'a', 'voice', 'mail', 'that', 'no', 'one', 'ever', 'answers', 'or', 'returns', 'your', 'call', 'both', 'my', 'adult', 'children', 'and', 'husband', 'have', 'decided', 'to', 'leave', 'this', 'practice', 'after', 'experiencing', 'such', 'frustration', 'the', 'entire', 'office', 'has', 'an', 'attitude', 'like', 'they', 'are', 'doing', 'you', 'a', 'favor', 'give', 'me', 'a', 'break', 'stay', 'away', 'from', 'this', 'doc', 'and', 'the', 'practice', 'you', 'deserve', 'better', 'and', 'they', 'will', 'not', 'be', 'there', 'when', 'you', 'really', 'need', 'them', 'i', 'have', 'never', 'felt', 'compelled', 'to', 'write', 'a', 'bad', 'review', 'about', 'anyone', 'until', 'i', 'met', 'this', 'pathetic', 'excuse', 'for', 'a', 'doctor', 'who', 'is', 'all', 'about', 'the', 'money']\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for filtered_review in filtered_reviews[:5]:\n",
    "    print(preprocess_text(filtered_review[0]))\n",
    "\n",
    "for filtered_review in reviews:\n",
    "    break\n",
    "    posTokens = nltk.word_tokenize(review)\n",
    "    properNounSearch = nltk.pos_tag(posTokens)\n",
    "    nounFlag = False\n",
    "    for _, tag in properNounSearch:\n",
    "        if tag == \"NNP\" or tag == \"NNPS\":\n",
    "            nounFlag = True\n",
    "    if not nounFlag:\n",
    "        continue\n",
    "    tokens = tokenizer.tokenize(clean_text(review))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    processedReview = []\n",
    "    for word, tag in tags:\n",
    "        lemma = lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
    "        if lemma not in english_stopwords and len(lemma) >= 2:\n",
    "            processedReview.append(lemma)\n",
    "    output.append(processedReview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', '<person>', 'offer', 'everything', 'look', 'general', 'practitioner', 'nice', 'easy', 'talk', 'without', 'patronize', 'always', 'time', 'see', 'patient', 'affiliate', 'topnotch', 'hospital', '<org>', 'parent', 'explain', 'important', 'case', 'something', 'happens', 'need', 'surgery', 'get', 'referral', 'see', 'specialist', 'without', 'see', 'first', 'really', 'need', 'im', 'sit', 'try', 'think', 'complaint', 'im', 'really', 'draw', 'blank']\n",
      "['unfortunately', 'frustration', 'dr', '<person>', 'patient', 'repeat', 'experience', 'ive', 'many', 'doctor', 'nyc', 'good', 'doctor', 'terrible', 'staff', 'seem', 'staff', 'simply', 'never', 'answer', 'phone', 'usually', 'take', 'hour', 'repeat', 'call', 'get', 'answer', 'time', 'want', 'deal', 'run', 'problem', 'many', 'doctor', 'dont', 'get', 'office', 'worker', 'patient', 'medical', 'need', 'isnt', 'anyone', 'answer', 'phone', 'incomprehensible', 'work', 'aggravation', 'regret', 'feel', 'give', 'dr', '<person>', 'star']\n",
      "['go', 'dr', '<person>', '10', 'year', 'think', 'one', '1st', 'patient', 'start', '<org>', 'great', 'year', 'really', 'big', 'picture', 'former', 'gyn', 'dr', '<person>', 'found', 'fibroid', 'explores', 'option', 'patient', 'understand', 'doesnt', 'judge', 'asks', 'right', 'question', 'thorough', 'want', 'kept', 'loop', 'every', 'aspect', 'medical', 'health', 'life']\n",
      "['get', 'letter', 'mail', 'last', 'week', 'say', 'dr', '<person>', 'move', 'arizona', 'take', 'new', 'position', 'june', 'miss', 'much', 'nni', 'think', 'find', 'new', 'doctor', 'nyc', 'actually', 'like', 'might', 'almost', 'awful', 'try', 'find', 'date']\n",
      "['dont', 'know', 'dr', '<person>', 'like', 'move', 'arizona', 'let', 'tell', 'stay', 'away', 'doctor', 'office', 'go', 'dr', '<person>', 'left', '<person>', 'take', '<person>', 'left', 'care', 'doctor', 'interested', 'copay', 'come', 'medication', 'refill', 'every', 'month', 'give', 'refill', 'could', 'less', 'patientss', 'financial', 'situation', 'try', 'get', '90', 'day', 'mail', 'away', 'pharmacy', 'prescription', 'guy', 'joke', 'make', 'matter', 'even', 'bad', 'office', 'staff', 'incompetent', '90', 'time', 'call', 'office', 'theyll', 'put', 'voice', 'mail', 'one', 'ever', 'answer', 'return', 'call', 'adult', 'child', 'husband', 'decide', 'leave', 'practice', 'experience', 'frustration', 'entire', 'office', 'attitude', 'like', 'favor', 'give', 'break', 'stay', 'away', 'doc', 'practice', 'deserve', 'well', 'really', 'need', 'never', 'felt', 'compel', 'write', 'bad', 'review', 'anyone', 'met', 'pathetic', 'excuse', 'doctor', 'money']\n"
     ]
    }
   ],
   "source": [
    "for filtered_review in filtered_reviews[:5]:\n",
    "    print(preprocess_text(filtered_review[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 'NN'),\n",
       " ('pasta', 'NN'),\n",
       " ('<org>', 'NNP'),\n",
       " ('especially', 'RB'),\n",
       " ('date', 'NN')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(preprocess_text(\"I love the pasta at <ORG>, especially on <DATE>.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<org>'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('<org>', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10701-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
