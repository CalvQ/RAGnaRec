{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'goldberg', 'offer', 'everything', 'look', 'general', 'practitioner', 'nice', 'easy', 'talk', 'without', 'patronizing', 'always', 'time', 'seeing', 'patient', 'affiliated', 'top', 'notch', 'hospital', 'nyu', 'parent', 'explained', 'important', 'case', 'something', 'happens', 'need', 'surgery', 'get', 'referral', 'see', 'specialist', 'without', 'see', 'first', 'really', 'need', 'sitting', 'trying', 'think', 'complaint', 'really', 'drawing', 'blank']\n"
     ]
    }
   ],
   "source": [
    "# get the cleaned reviews \n",
    "with open(\"clean_reviews.pkl\", \"rb\") as file:\n",
    "    clean_reviews = pkl.load(file)\n",
    "    print(clean_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"clean_review_corpus.pkl\", \"rb\") as file:\n",
    "#     corpus = pkl.load(file)\n",
    "#     print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsample Reviews for Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_reviews))\n",
    "# index_list = range(len(clean_reviews))\n",
    "# random.sample(index_list, 10000)\n",
    "random.seed(10701)\n",
    "clean_reviews_down = random.sample(clean_reviews, 10000)\n",
    "print(len(clean_reviews_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was the first place in Vegas where the Yelpers let me down. The salt and pepper shrimp appetizer was inedible: all of the shell was left on underneath the crispy fried coating. As a result, the sauce used to marinate the shrimp never penetrated the meat, it coagulated in the head, and EXPLODED on you upon taking a bite. GROSS! The potstickers and the beef chow fun lacked any flavor whatsoever. The only redeeming quality (the service is pretty awful) is the roast duck: crispy and tasty skin with relatively moist meat and a relative bargain compared to places in NY and L.A.\n"
     ]
    }
   ],
   "source": [
    "# get original reviews\n",
    "with open(\"reviews.pkl\", \"rb\") as file:\n",
    "    reviews = pkl.load(file)\n",
    "    print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab_list\n",
    "dictionary = Dictionary(clean_reviews_down)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "vocab_list = list(dictionary.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vocabulary.pkl\", \"wb\") as file:\n",
    "#     pkl.dump(vocab_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vocabulary.pkl\", \"rb\") as file:\n",
    "#     vocab_list = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(processed_text, vocabulary=vocab_list):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    dtm = vectorizer.fit_transform(processed_text)\n",
    "    return dtm, vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = [\" \".join(tokens) for tokens in clean_reviews_down]\n",
    "dtm, vocab_dict = create_matrix(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_TOPIC = 7             # number of clusters\n",
    "N_ITER = 1000           # number of iterations run on dataset\n",
    "RANDOM_STATE = 7        # initial random seed\n",
    "REFRESH = 20            # how often you print log-likelihood\n",
    "SEED_CONFIDENCE=0.5     # probability of using this word as a seed\n",
    "\n",
    "# create topic seeds\n",
    "# seed_topic_list = [\n",
    "#     [\"medicine\", \"office\", \"hurt\"],\n",
    "#     [\"coffee\", \"pizza\", \"delicious\"]\n",
    "# ]\n",
    "seed_topic_list = [\n",
    "    [\"pizza\", \"food\", \"chicken\", \"burger\", \"taco\", \"salsa\", \"mexican\", \"chip\", \"bean\", \"burrito\", \"enchilada\", \"rice\", \"tortilla\", \"guacamole\", \"carne\", \"asada\", \"shrimp\", \"lobster\", \"sushi\", \"roll\", \"fish\", \"sashimi\", \"tuna\", \"tempura\"],\n",
    "    [\"doctor\", \"office\", \"medical\", \"service\", \"patient\", \"health\", \"insurance\", \"client\", \"car\", \"vehicle\"], \n",
    "    [\"hair\", \"nail\", \"salon\", \"gel\", \"polish\", \"manicure\"], #maybe counts as service? \n",
    "    \"de le la u00e9 et est u00e0 un com pa www que da en die yelp u00e9e du select une pour service au mais plus\".split(\" \"), #french\n",
    "    \"bar drink beer night bartender table music atmosphere\".split(\" \"), #bars\n",
    "    \"hotel bed bath shower spa pool casino strip night desk check club show sexy men girl\".split(\" \"),#vegas\n",
    "    \"store shop price item shopping buy need sale mall product boutique outlet walmart\".split(\" \"),#shopping malls\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "seed_topics = {}\n",
    "for topic_id, topic_words in enumerate(seed_topic_list):\n",
    "    for word in topic_words:\n",
    "        seed_topics[vocab_dict[word]] = topic_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = guidedlda.GuidedLDA(n_topics=N_TOPIC, n_iter=N_ITER, random_state=RANDOM_STATE, refresh=REFRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:vocab_size: 3266\n",
      "INFO:guidedlda:n_words: 581744\n",
      "INFO:guidedlda:n_topics: 7\n",
      "INFO:guidedlda:n_iter: 1000\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -5641643\n",
      "INFO:guidedlda:<20> log likelihood: -4513566\n",
      "INFO:guidedlda:<40> log likelihood: -4450729\n",
      "INFO:guidedlda:<60> log likelihood: -4423268\n",
      "INFO:guidedlda:<80> log likelihood: -4407645\n",
      "INFO:guidedlda:<100> log likelihood: -4398073\n",
      "INFO:guidedlda:<120> log likelihood: -4389204\n",
      "INFO:guidedlda:<140> log likelihood: -4383532\n",
      "INFO:guidedlda:<160> log likelihood: -4380337\n",
      "INFO:guidedlda:<180> log likelihood: -4378016\n",
      "INFO:guidedlda:<200> log likelihood: -4372005\n",
      "INFO:guidedlda:<220> log likelihood: -4370342\n",
      "INFO:guidedlda:<240> log likelihood: -4367901\n",
      "INFO:guidedlda:<260> log likelihood: -4365128\n",
      "INFO:guidedlda:<280> log likelihood: -4364856\n",
      "INFO:guidedlda:<300> log likelihood: -4361293\n",
      "INFO:guidedlda:<320> log likelihood: -4360134\n",
      "INFO:guidedlda:<340> log likelihood: -4357725\n",
      "INFO:guidedlda:<360> log likelihood: -4355782\n",
      "INFO:guidedlda:<380> log likelihood: -4353014\n",
      "INFO:guidedlda:<400> log likelihood: -4350928\n",
      "INFO:guidedlda:<420> log likelihood: -4351134\n",
      "INFO:guidedlda:<440> log likelihood: -4349410\n",
      "INFO:guidedlda:<460> log likelihood: -4344523\n",
      "INFO:guidedlda:<480> log likelihood: -4345906\n",
      "INFO:guidedlda:<500> log likelihood: -4343307\n",
      "INFO:guidedlda:<520> log likelihood: -4343729\n",
      "INFO:guidedlda:<540> log likelihood: -4342316\n",
      "INFO:guidedlda:<560> log likelihood: -4340085\n",
      "INFO:guidedlda:<580> log likelihood: -4339899\n",
      "INFO:guidedlda:<600> log likelihood: -4337889\n",
      "INFO:guidedlda:<620> log likelihood: -4336938\n",
      "INFO:guidedlda:<640> log likelihood: -4336879\n",
      "INFO:guidedlda:<660> log likelihood: -4335027\n",
      "INFO:guidedlda:<680> log likelihood: -4335033\n",
      "INFO:guidedlda:<700> log likelihood: -4332773\n",
      "INFO:guidedlda:<720> log likelihood: -4333674\n",
      "INFO:guidedlda:<740> log likelihood: -4331833\n",
      "INFO:guidedlda:<760> log likelihood: -4331434\n",
      "INFO:guidedlda:<780> log likelihood: -4332317\n",
      "INFO:guidedlda:<800> log likelihood: -4329705\n",
      "INFO:guidedlda:<820> log likelihood: -4329191\n",
      "INFO:guidedlda:<840> log likelihood: -4328968\n",
      "INFO:guidedlda:<860> log likelihood: -4328031\n",
      "INFO:guidedlda:<880> log likelihood: -4328069\n",
      "INFO:guidedlda:<900> log likelihood: -4326746\n",
      "INFO:guidedlda:<920> log likelihood: -4327185\n",
      "INFO:guidedlda:<940> log likelihood: -4324204\n",
      "INFO:guidedlda:<960> log likelihood: -4323202\n",
      "INFO:guidedlda:<980> log likelihood: -4322998\n",
      "INFO:guidedlda:<999> log likelihood: -4322550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x3a810bf10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(dtm, seed_topics=seed_topics, seed_confidence=SEED_CONFIDENCE)\n",
    "# model.fit(dtm, seed_confidence=SEED_CONFIDENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Topic 0: good food place like great really restaurant chicken one time would go ordered service get sauce menu back also ni nthe cheese try salad pizza\n",
      "Topic 1: would back time get said told service customer one never call even day called go could asked got like know went ni manager phone car\n",
      "Topic 2: great nail time place hair massage salon service like get good back staff always done appointment go nice job friendly cut recommend would went really\n",
      "Topic 3: breakfast coffee cream egg de cake ice le chocolate tea cupcake la sweet bacon pancake brunch bagel u00e9 et toast flavor morning fruit waffle delicious\n",
      "Topic 4: food service table time drink minute order get place server one back came ordered good restaurant would got asked even took bar wait go went\n",
      "Topic 5: room hotel vega night like place get one nice show stay good time nthe would strip great casino floor pool really people go got club\n",
      "Topic 6: place one like time great good get go really ha food store always love lot price back nice bar little ni nthe people well make\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "print(len(topic_word))\n",
    "n_top_words = 25\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # print(topic_dist)\n",
    "    topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Topic given Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   7,   14,   26,   55,   58,   66,   86,  143,  149,  167,  176,\n",
      "        177,  184,  190,  210,  219,  240,  242,  279,  301,  311,  320,\n",
      "        321,  329,  331,  333,  339,  341,  376,  382,  388,  395,  401,\n",
      "        404,  414,  428,  429,  456,  511,  512,  516,  527,  571,  572,\n",
      "        580,  590,  609,  614,  631,  648,  661,  683,  689,  691,  697,\n",
      "        700,  721,  736,  760,  783,  787,  798,  801,  834,  872,  883,\n",
      "        909,  957,  979,  988, 1002, 1019, 1030, 1096, 1102, 1105, 1124,\n",
      "       1143, 1155, 1156, 1158, 1159, 1185, 1192, 1210, 1219, 1236, 1243,\n",
      "       1250, 1253, 1255, 1259, 1272, 1289, 1291, 1303, 1310, 1319, 1323,\n",
      "       1350, 1358, 1359, 1381, 1411, 1418, 1439, 1489, 1495, 1497, 1515,\n",
      "       1522, 1555, 1614, 1636, 1638, 1641, 1652, 1745, 1831, 1838, 1856,\n",
      "       1861, 1865, 1882, 1896, 1897, 1927, 1931, 1945, 1946, 1962, 1968,\n",
      "       1969, 1977, 1989, 2000, 2006, 2015, 2034, 2035, 2056, 2068, 2090,\n",
      "       2092, 2105, 2114, 2118, 2140, 2154, 2177, 2188, 2199, 2216, 2222,\n",
      "       2228, 2230, 2235, 2236, 2258, 2299, 2303, 2308, 2323, 2333, 2337,\n",
      "       2352, 2380, 2417, 2456, 2482, 2497, 2508, 2509, 2556, 2577, 2594,\n",
      "       2621, 2622, 2623, 2626, 2649, 2656, 2663, 2687, 2689, 2698, 2726,\n",
      "       2760, 2764, 2768, 2774, 2796, 2811, 2816, 2829, 2833, 2843, 2853,\n",
      "       2856, 2867, 2882, 2896, 2920, 2925, 2933, 2934, 2991, 2999, 3013,\n",
      "       3024, 3033, 3036, 3048, 3064, 3090, 3109, 3110, 3125, 3126, 3144,\n",
      "       3147, 3171, 3172, 3175, 3257, 3276, 3280, 3331, 3339, 3341, 3347,\n",
      "       3355, 3376, 3435, 3459, 3460, 3472, 3551, 3576, 3607, 3608, 3625,\n",
      "       3688, 3705, 3707, 3710, 3733, 3737, 3743, 3744, 3746, 3762, 3780,\n",
      "       3781, 3796, 3839, 3879, 3889, 3890, 3915, 3935, 3937, 3944, 3957,\n",
      "       3971, 3997, 4008, 4017, 4031, 4039, 4048, 4052, 4074, 4075, 4089,\n",
      "       4097, 4121, 4127, 4130, 4167, 4173, 4185, 4188, 4196, 4211, 4234,\n",
      "       4248, 4275, 4284, 4305, 4329, 4335, 4379, 4380, 4383, 4435, 4444,\n",
      "       4460, 4463, 4514, 4520, 4523, 4558, 4567, 4586, 4599, 4606, 4616,\n",
      "       4621, 4640, 4651, 4663, 4676, 4712, 4720, 4760, 4764, 4771, 4819,\n",
      "       4835, 4873, 4880, 4881, 4903, 4919, 4923, 4925, 4929, 4931, 4941,\n",
      "       4970, 4996, 5008, 5045, 5050, 5084, 5108, 5127, 5178, 5201, 5205,\n",
      "       5212, 5246, 5261, 5267, 5301, 5343, 5401, 5415, 5419, 5427, 5438,\n",
      "       5460, 5464, 5473, 5476, 5484, 5516, 5528, 5529, 5533, 5556, 5559,\n",
      "       5589, 5592, 5596, 5602, 5616, 5634, 5655, 5660, 5666, 5694, 5700,\n",
      "       5718, 5739, 5751, 5759, 5779, 5784, 5796, 5811, 5838, 5850, 5912,\n",
      "       5925, 5935, 5948, 5961, 5970, 5977, 6018, 6026, 6053, 6061, 6063,\n",
      "       6067, 6077, 6097, 6129, 6140, 6143, 6188, 6208, 6218, 6230, 6249,\n",
      "       6263, 6278, 6309, 6312, 6315, 6321, 6347, 6351, 6360, 6361, 6416,\n",
      "       6427, 6430, 6431, 6444, 6446, 6453, 6463, 6465, 6499, 6539, 6562,\n",
      "       6566, 6603, 6615, 6673, 6690, 6691, 6711, 6721, 6725, 6734, 6750,\n",
      "       6783, 6806, 6809, 6814, 6825, 6840, 6842, 6846, 6849, 6876, 6892,\n",
      "       6916, 6955, 6969, 6974, 6992, 7020, 7027, 7047, 7073, 7086, 7093,\n",
      "       7099, 7134, 7141, 7152, 7165, 7184, 7214, 7226, 7258, 7262, 7339,\n",
      "       7350, 7353, 7354, 7357, 7365, 7415, 7418, 7431, 7434, 7471, 7473,\n",
      "       7479, 7495, 7531, 7539, 7543, 7544, 7604, 7608, 7610, 7615, 7635,\n",
      "       7678, 7682, 7688, 7708, 7760, 7762, 7767, 7768, 7783, 7790, 7797,\n",
      "       7812, 7813, 7814, 7816, 7852, 7861, 7903, 7915, 7922, 7923, 7936,\n",
      "       7959, 7962, 7966, 7991, 7998, 8007, 8039, 8083, 8096, 8111, 8124,\n",
      "       8138, 8179, 8204, 8205, 8233, 8237, 8257, 8267, 8272, 8310, 8369,\n",
      "       8379, 8387, 8406, 8413, 8454, 8463, 8480, 8500, 8505, 8528, 8535,\n",
      "       8590, 8596, 8630, 8647, 8654, 8674, 8690, 8714, 8741, 8746, 8749,\n",
      "       8772, 8778, 8780, 8786, 8789, 8797, 8802, 8805, 8822, 8825, 8862,\n",
      "       8866, 8898, 8925, 8927, 8949, 8962, 8967, 8968, 8969, 9011, 9035,\n",
      "       9042, 9048, 9049, 9056, 9064, 9077, 9081, 9110, 9128, 9138, 9186,\n",
      "       9261, 9265, 9268, 9328, 9329, 9377, 9385, 9416, 9419, 9492, 9496,\n",
      "       9497, 9535, 9652, 9664, 9697, 9705, 9750, 9764, 9783, 9791, 9795,\n",
      "       9810, 9824, 9830, 9832, 9879, 9882, 9887, 9892, 9894, 9895, 9954,\n",
      "       9967, 9969, 9972, 9980]),)\n"
     ]
    }
   ],
   "source": [
    "dtm_transform = model.transform(dtm)\n",
    "\n",
    "dtm_topics = np.argmax(dtm_transform, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 14 26 55 58]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[ 220  395  399  407  406  423  421  110  396  412  400  415  411  413\n",
      "   414]\n",
      " [ 372    1  142  683  140   28  685  392  684  119  678  199  682  681\n",
      "    89]\n",
      " [  52  411  963  904  409  966  603  962  964  965  967  266  968  969\n",
      "   902]\n",
      " [ 142  372   89  834   52  397 1408  362  268   29  202   98  413 1193\n",
      "   574]\n",
      " [ 220   52  834  372   92  351   28  143 1430  912   27 1244 1295 1180\n",
      "  1432]]\n",
      "[['good' 'beer' 'brunch' 'flatbread' 'dog' 'patio' 'opted' 'my' 'biscuit'\n",
      "  'gravy' 'calling' 'grilled' 'fry' 'great' 'greatest']\n",
      " ['pizza' 'all' 'we' 'own' 'very' 'on' 'saw' 'than' 'premium' 'one'\n",
      "  'agreed' 'ever' 'mixed' 'ingredient' 'had']\n",
      " ['with' 'fry' 'lunch' 'sub' 'french' 'reasonable' 'nothing' 'free'\n",
      "  'mention' 'presented' 'recently' 'or' 'soda' 'turkey' 'run']\n",
      " ['we' 'pizza' 'had' 'crust' 'with' 'bread' 'delivered' 'cheese'\n",
      "  'ordered' 'only' 'extra' 'into' 'great' 'recommended' 'square']\n",
      " ['good' 'with' 'crust' 'pizza' 'here' 'still' 'on' 'weekend' 'general'\n",
      "  'during' 'ny' 'various' 'slice' 'isn' 'spectacular']]\n",
      "good beer brunch flatbread dog patio opted my biscuit gravy calling grilled fry great greatest\n",
      "pizza all we own very on saw than premium one agreed ever mixed ingredient had\n",
      "with fry lunch sub french reasonable nothing free mention presented recently or soda turkey run\n",
      "we pizza had crust with bread delivered cheese ordered only extra into great recommended square\n",
      "good with crust pizza here still on weekend general during ny various slice isn spectacular\n"
     ]
    }
   ],
   "source": [
    "vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "for topic_idx in range(N_TOPIC):\n",
    "    # Get 5 example documents\n",
    "    topic_idx_docs = np.where(dtm_topics == topic_idx)[0][:5]\n",
    "    print(topic_idx_docs)\n",
    "    doc_dtm = dtm[topic_idx_docs].toarray()\n",
    "    print(doc_dtm)\n",
    "    # Get 15 terms from each document\n",
    "    common_terms = np.argsort(doc_dtm, axis=1)[:, ::-1][:,:15]\n",
    "    print(common_terms)\n",
    "    print(vocab_map(common_terms))\n",
    "    document_words = vocab_map(common_terms)\n",
    "    for doc in document_words:\n",
    "        print(\" \".join(doc))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x31595 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 75714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 1000 reviews\n",
    "item_term_matrix, _ = create_matrix(processed_corpus[:1000])\n",
    "item_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 20  6 20  0  8  8 24  1 24 14 24  6  6  2  8 24 19  1 19 19 19 19 23\n",
      " 13  6 17 18 20 20  1  0  3  4 17 15 15 15 15  2 15 17 18  9 13 15  0 15\n",
      " 20  1]\n",
      "[ 4 31 46]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[  323   332   169 ... 21069 21070 15797]\n",
      " [ 1666   554   190 ... 21069 21070     0]\n",
      " [ 1137   243   612 ... 21069 21070     0]]\n",
      "[[ 323  332  169  338   70]\n",
      " [1666  554  190  171  586]\n",
      " [1137  243  612  169 2057]]\n",
      "[['hot' 'sauce' 'are' 'wing' 'good']\n",
      " ['chinese' 'food' 'even' 'away' 'taste']\n",
      " ['soup' 'best' 'around' 'are' 'chow']]\n",
      "Top topic: 20, Document terms: he, you, him, need, really, have, see, without, parent, referral, patient, on, patronizing, offer, practitioner\n",
      "Top topic: 20, Document terms: he, all, very, been, over, your, dr, patient, year, out, think, health, option, question, one\n",
      "Top topic: 6, Document terms: you, office, he, doctor, not, when, about, away, they, me, call, johnson, will, before, practice\n",
      "Top topic: 20, Document terms: have, dr, doctor, his, very, we, him, many, who, come, been, ha, diagnosed, every, had\n",
      "Top topic: 0, Document terms: hot, sauce, are, wing, good, fish, extra, flavor, frank, red, heat, decent, large, lot, maybe\n",
      "Top topic: 8, Document terms: out, ball, range, are, driving, place, they, you, money, only, sell, light, make, let, mat\n",
      "Top topic: 8, Document terms: look, wait, range, can, on, tee, state, turf, be, yesterday, amazing, nice, ll, really, re\n",
      "Top topic: 24, Document terms: sandwich, were, reuben, fish, on, we, had, they, very, pretty, time, giant, be, extremely, recommendation\n",
      "Top topic: 1, Document terms: you, are, all, place, on, food, re, don, if, good, they, one, get, nthe, lot\n",
      "Top topic: 24, Document terms: good, fish, sandwich, lobsicle, spain, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced\n",
      "Top topic: 14, Document terms: mushroom, an, from, canned, place, they, ever, tasty, know, crowd, were, pizza, staff, bar, night\n",
      "Top topic: 24, Document terms: coleslaw, we, kraut, very, they, charged, be, disappointed, instead, customer, tasting, back, reuben, not, iced\n",
      "Top topic: 6, Document terms: tire, wait, get, they, toyota, there, price, hour, new, good, long, week, lost, make, looked\n",
      "Top topic: 6, Document terms: had, they, tire, called, them, overpriced, seem, summer, super, on, didn, going, also, time, very\n",
      "Top topic: 2, Document terms: lobsicle, sauv, unwanted, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced, prawn, spain\n",
      "Top topic: 8, Document terms: really, don, found, associate, luckily, mill, puppy, teen, treat, on, moved, am, around, young, they\n",
      "Top topic: 24, Document terms: fish, home, good, their, ordered, served, friend, rocky, fry, ate, meatball, brought, he, style, food\n",
      "Top topic: 19, Document terms: eat, definition, sausage, very, fry, here, you, unpretentious, put, thank, country, joint, favorite, home, diner\n",
      "Top topic: 1, Document terms: line, great, ready, stand, people, bad, place, be, meal, never, love, had, portion, good, innovation\n",
      "Top topic: 19, Document terms: you, are, best, place, they, make, healthy, on, grill, breakfast, like, diner, we, many, yourself\n",
      "Top topic: 19, Document terms: they, had, good, made, now, return, toast, what, eat, great, fresh, french, turkey, grill, wait\n",
      "Top topic: 19, Document terms: service, friendly, poached, only, what, eat, crispy, bacon, felt, south, family, frankly, homefries, nfantastic, gab\n",
      "Top topic: 19, Document terms: had, place, nthe, give, really, were, some, decent, price, food, country, which, good, on, service\n",
      "Top topic: 23, Document terms: people, tonya, are, front, very, sweet, desk, helpful, super, prawn, barn, brioni, 5k, measly, influenced\n",
      "Top topic: 13, Document terms: irish, again, bar, great, food, best, not, doe, better, than, place, town, what, nor, service\n",
      "Top topic: 6, Document terms: than, order, phone, his, on, other, he, would, rather, driver, town, just, or, speed, call\n",
      "Top topic: 17, Document terms: were, from, pizza, if, so, ordered, about, there, them, put, no, on, sicilian, bad, meat\n",
      "Top topic: 18, Document terms: you, stuff, are, fried, review, after, really, regret, finish, eating, ljs, cardboard, minute, boat, extra\n",
      "Top topic: 20, Document terms: her, have, had, not, all, them, she, too, tooth, an, because, on, me, care, follow\n",
      "Top topic: 20, Document terms: me, at, review, did, office, door, not, nice, distracted, very, where, there, you, dentist, waited\n",
      "Top topic: 1, Document terms: restaurant, there, favorite, don, food, eat, average, few, myself, carnegie, eating, place, think, service, had\n",
      "Top topic: 0, Document terms: chinese, food, even, away, taste, style, doesn, from, far, real, good, american, ballet, 5k, barn\n",
      "Top topic: 3, Document terms: we, minute, not, meal, came, food, our, an, order, drink, or, so, place, at, waited\n",
      "Top topic: 4, Document terms: cardio, only, you, equipment, gym, room, me, weight, us, training, session, section, personal, join, increase\n",
      "Top topic: 17, Document terms: gym, get, working, egg, try, she, by, if, instead, nasty, most, morning, complain, you, worker\n",
      "Top topic: 15, Document terms: had, time, experience, bread, sandwich, mixed, papa, first, girlfriend, take, ve, server, me, fish, care\n",
      "Top topic: 15, Document terms: white, pizza, bar, order, calamari, go, on, visit, note, nlike, our, ever, ok, style, very\n",
      "Top topic: 15, Document terms: nthe, had, pizza, calamari, white, we, take, time, garlic, very, order, style, some, fried, than\n",
      "Top topic: 15, Document terms: food, everyone, serving, yummy, great, dessert, be, gem, little, place, very, try, without, you, price\n",
      "Top topic: 2, Document terms: liked, place, or, down, great, because, be, california, imagine, relocating, crazy, street, stroll, visitor, couple\n",
      "Top topic: 15, Document terms: restaurant, dish, had, were, on, ordered, made, at, zucchini, not, chicken, marsala, which, she, too\n",
      "Top topic: 17, Document terms: terrible, like, wood, smell, place, wet, rotten, service, food, barn, brioni, lobsicle, prawn, 5k, measly\n",
      "Top topic: 18, Document terms: justify, inconsistent, there, expensive, nice, carnegie, restaurant, price, better, ambiance, are, papa, doesn, spain, innovation\n",
      "Top topic: 9, Document terms: had, we, manager, service, minute, want, you, said, back, get, go, bill, do, our, your\n",
      "Top topic: 13, Document terms: always, place, fresh, so, love, well, food, friendly, staff, delicious, douche, prawn, corresponding, nsuper, caterer\n",
      "Top topic: 15, Document terms: had, back, great, size, on, really, pepper, would, very, what, enough, good, green, just, sauce\n",
      "Top topic: 0, Document terms: soup, best, around, are, chow, salad, some, place, great, bread, love, 5k, innovation, measly, brioni\n",
      "Top topic: 15, Document terms: at, pasta, had, seriously, we, service, went, could, rather, down, hill, price, eat, she, wine\n",
      "Top topic: 20, Document terms: they, have, will, going, nice, be, behind, them, are, stand, re, rude, ve, unprofessional, what\n",
      "Top topic: 1, Document terms: dental, been, you, office, should, time, no, beat, worst, avoid, at, ever, can, any, one\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(item_term_matrix)\n",
    "\n",
    "top_topic = np.argmax(doc_topic, axis=1) #find the topic of the document\n",
    "print(top_topic)\n",
    "\n",
    "for topic_idx in range(N_TOPIC):\n",
    "    # Grab 5 example documents from each topic\n",
    "    topic_idx_docs = np.where(top_topic == topic_idx)[:5]\n",
    "    doc_dtm = item_term_matrix[topic_idx_docs].toarray()\n",
    "    \n",
    "\n",
    "doc_topics = [np.where(top_topic == topic_idx) for topic_idx in N_TOPIC] # a list of list where the [i][.] corresponds to list of document in topic i\n",
    "topic_doc_indices = np.where(top_topic == 0)[0][:5] #find documents from topic 0\n",
    "print(topic_doc_indices)\n",
    "print(item_term_matrix[topic_doc_indices].toarray()) #print document term matrix for those documents\n",
    "print(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1]) #argsort to find the most common terms\n",
    "print(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1][:, :5]) #same thing, gives top 5\n",
    "\n",
    "vectorized_vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "print(vectorized_vocab_map(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1][:, :5])) #get words corresponding to indices\n",
    "\n",
    "for i in range(len(doc_topic)):  # Use the correct range based on your data\n",
    "    # Get the top topic for the document\n",
    "    top_topic = doc_topic[i].argmax()\n",
    "    \n",
    "    # Get the top terms more frequent than 1 in the document from the dtm\n",
    "    row = item_term_matrix[i, :].toarray().flatten()  # Convert sparse row to dense\n",
    "    # item_count = np.count_nonzero(row > 1)\n",
    "    top_terms_indices = row.argsort()[::-1][:15]  # Indices of the top 10 terms\n",
    "    top_terms = [vocab_list[idx] for idx in top_terms_indices]\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Top topic: {top_topic}, Document terms: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dataset = []\n",
    "topic_review = [] # reviews in topics \n",
    "\n",
    "output_review_num = 5 # output the top 5 reviwew from each topic\n",
    "\n",
    "for topic in range(N_TOPIC):\n",
    "    if len(doc_topics[topic]) >= 5:\n",
    "        end = 5\n",
    "    else:\n",
    "        end = len(doc_topics[topic])\n",
    "    for review_idx in doc_topics[topic][:end]:\n",
    "        topic_review[topic].append(og_dataset[review_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try making code to extract 5 documents per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prunes matrices, saves weights but locks model\n",
    "# model.purge_extra_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = f\"model_{N_TOPIC}_{N_ITER}_{RANDOM_STATE}_{REFRESH}_{SEED_CONFIDENCE}.pkl\"\n",
    "\n",
    "with open(f\"results/{modelname}\", 'wb') as file:\n",
    "    pkl.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.91332837e-01, 6.84035745e-04, 6.00474419e-03, 1.73764585e-04,\n",
       "        3.63391083e-04, 9.31279092e-04, 5.09947976e-04]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the model\n",
    "with open('results/model_7_1000_7_20_0.5.pickle', 'rb') as file:\n",
    "    model = pkl.load(file)\n",
    "model.transform(create_matrix(processed_corpus[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-768290932.3669106,\n",
       " -544845646.4478563,\n",
       " -523167504.4543283,\n",
       " -517464505.4454495,\n",
       " -515242153.73157394,\n",
       " -514171247.29473853,\n",
       " -513573261.2712008,\n",
       " -513116435.8723612]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loglikelihoods_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "\n",
    "Make sure to run this after each experiment (after you've created/fitted the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in result_15_500_7_20_0.5.txt\n"
     ]
    }
   ],
   "source": [
    "filename = f\"result_{N_TOPIC}_{N_ITER}_{RANDOM_STATE}_{REFRESH}_{SEED_CONFIDENCE}.txt\"\n",
    "\n",
    "with open(f\"results/{filename}\", \"w\") as f:\n",
    "    if seed_topic_list:\n",
    "        for topic_list in seed_topic_list:\n",
    "            print(f\"{topic_list}\", file=f)\n",
    "        print(file=f)\n",
    "    else:\n",
    "        print(\"No Seeds\\n\", file=f)\n",
    "        \n",
    "    dtm_transform = model.transform(dtm)\n",
    "    dtm_topics = np.argmax(dtm_transform, axis=1)\n",
    "    vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "\n",
    "    # Save top 25 words per topic\n",
    "    topic_word = model.topic_word_\n",
    "    n_top_words = 25\n",
    "    for idx, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(idx, ' '.join(topic_words)), file=f)\n",
    "        print(\"Example Documents:\", file=f)\n",
    "        \n",
    "        # Get 5 example documents\n",
    "        topic_idx_docs = np.where(dtm_topics == idx)[0][:5]\n",
    "        \n",
    "        doc_dtm = dtm[topic_idx_docs].toarray()\n",
    "        \n",
    "        # Get 15 terms from each document\n",
    "        common_terms = np.argsort(doc_dtm, axis=1)[:, ::-1][:,:15]\n",
    "        document_words = vocab_map(common_terms)\n",
    "        for doc in document_words:\n",
    "            print(\" \".join(doc), file=f)\n",
    "        \n",
    "        # Newline as separator\n",
    "        print(file=f)\n",
    "    \n",
    "\n",
    "print(\"Saved in\", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10701-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
