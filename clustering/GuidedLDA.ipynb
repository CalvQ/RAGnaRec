{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidedlda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpora\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dictionary\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpkl\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mguidedlda\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'guidedlda'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'goldberg', 'offer', 'everything', 'look', 'general', 'practitioner', 'nice', 'easy', 'talk', 'without', 'patronizing', 'always', 'time', 'seeing', 'patient', 'affiliated', 'top', 'notch', 'hospital', 'nyu', 'parent', 'explained', 'important', 'case', 'something', 'happens', 'need', 'surgery', 'get', 'referral', 'see', 'specialist', 'without', 'see', 'first', 'really', 'need', 'sitting', 'trying', 'think', 'complaint', 'really', 'drawing', 'blank']\n"
     ]
    }
   ],
   "source": [
    "# get the cleaned reviews \n",
    "with open(\"clean_reviews.pkl\", \"rb\") as file:\n",
    "    clean_reviews = pkl.load(file)\n",
    "    print(clean_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 3)]\n"
     ]
    }
   ],
   "source": [
    "# with open(\"clean_review_corpus.pkl\", \"rb\") as file:\n",
    "#     corpus = pkl.load(file)\n",
    "#     print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsample Reviews for Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_reviews))\n",
    "random.seed(10701)\n",
    "clean_reviews_down = random.sample(clean_reviews, 10000)\n",
    "print(len(clean_reviews_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was the first place in Vegas where the Yelpers let me down. The salt and pepper shrimp appetizer was inedible: all of the shell was left on underneath the crispy fried coating. As a result, the sauce used to marinate the shrimp never penetrated the meat, it coagulated in the head, and EXPLODED on you upon taking a bite. GROSS! The potstickers and the beef chow fun lacked any flavor whatsoever. The only redeeming quality (the service is pretty awful) is the roast duck: crispy and tasty skin with relatively moist meat and a relative bargain compared to places in NY and L.A.\n"
     ]
    }
   ],
   "source": [
    "# get original reviews\n",
    "with open(\"reviews.pkl\", \"rb\") as file:\n",
    "    reviews = pkl.load(file)\n",
    "    print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wa',\n",
       " 'first',\n",
       " 'place',\n",
       " 'vega',\n",
       " 'yelpers',\n",
       " 'let',\n",
       " 'salt',\n",
       " 'pepper',\n",
       " 'shrimp',\n",
       " 'appetizer',\n",
       " 'wa',\n",
       " 'inedible',\n",
       " 'shell',\n",
       " 'wa',\n",
       " 'left',\n",
       " 'underneath',\n",
       " 'crispy',\n",
       " 'fried',\n",
       " 'coating',\n",
       " 'result',\n",
       " 'sauce',\n",
       " 'used',\n",
       " 'marinate',\n",
       " 'shrimp',\n",
       " 'never',\n",
       " 'penetrated',\n",
       " 'meat',\n",
       " 'coagulated',\n",
       " 'head',\n",
       " 'exploded',\n",
       " 'upon',\n",
       " 'taking',\n",
       " 'bite',\n",
       " 'gross',\n",
       " 'potstickers',\n",
       " 'beef',\n",
       " 'chow',\n",
       " 'fun',\n",
       " 'lacked',\n",
       " 'flavor',\n",
       " 'whatsoever',\n",
       " 'redeeming',\n",
       " 'quality',\n",
       " 'service',\n",
       " 'pretty',\n",
       " 'awful',\n",
       " 'roast',\n",
       " 'duck',\n",
       " 'crispy',\n",
       " 'tasty',\n",
       " 'skin',\n",
       " 'relatively',\n",
       " 'moist',\n",
       " 'meat',\n",
       " 'relative',\n",
       " 'bargain',\n",
       " 'compared',\n",
       " 'place',\n",
       " 'ny']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_down[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<28445 unique tokens: ['appetizer', 'awful', 'bargain', 'beef', 'bite']...> from 10000 documents (total 692701 corpus positions)\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<28445 unique tokens: ['appetizer', 'awful', 'bargain', 'beef', 'bite']...> from 10000 documents (total 692701 corpus positions)\", 'datetime': '2024-11-19T15:28:13.794028', 'gensim': '4.3.3', 'python': '3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]', 'platform': 'macOS-14.7-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO:gensim.corpora.dictionary:discarding 25179 tokens: [('coagulated', 1), ('coating', 12), ('exploded', 3), ('marinate', 2), ('penetrated', 1), ('potstickers', 8), ('redeeming', 18), ('wa', 6517), ('4ft', 1), ('bridge', 9)]...\n",
      "INFO:gensim.corpora.dictionary:keeping 3266 tokens which were in no less than 20 and no more than 5000 (=50.0%) documents\n",
      "INFO:gensim.corpora.dictionary:resulting dictionary: Dictionary<3266 unique tokens: ['appetizer', 'awful', 'bargain', 'beef', 'bite']...>\n"
     ]
    }
   ],
   "source": [
    "# get the vocab_list\n",
    "dictionary = Dictionary(clean_reviews_down)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "vocab_list = list(dictionary.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vocabulary.pkl\", \"wb\") as file:\n",
    "#     pkl.dump(vocab_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vocabulary.pkl\", \"rb\") as file:\n",
    "#     vocab_list = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(processed_text, vocabulary=vocab_list):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    dtm = vectorizer.fit_transform(processed_text)\n",
    "    return dtm, vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wa first place vega yelpers let salt pepper shrimp appetizer wa inedible shell wa left underneath crispy fried coating result sauce used marinate shrimp never penetrated meat coagulated head exploded upon taking bite gross potstickers beef chow fun lacked flavor whatsoever redeeming quality service pretty awful roast duck crispy tasty skin relatively moist meat relative bargain compared place ny\n"
     ]
    }
   ],
   "source": [
    "processed_corpus = [\" \".join(tokens) for tokens in clean_reviews_down]\n",
    "print(processed_corpus[0])\n",
    "dtm, vocab_dict = create_matrix(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_TOPIC = 25            # number of clusters\n",
    "N_ITER = 500            # number of iterations run on dataset\n",
    "RANDOM_STATE = 7        # initial random seed\n",
    "REFRESH = 20            # how often you print log-likelihood\n",
    "SEED_CONFIDENCE=0.15    # probability of using this word as a seed\n",
    "\n",
    "# create topic seeds\n",
    "# seed_topic_list = [\n",
    "#     [\"medicine\", \"office\", \"hurt\"],\n",
    "#     [\"coffee\", \"pizza\", \"delicious\"]\n",
    "# ]\n",
    "seed_topic_list = [\n",
    "    [\"pizza\", \"food\", \"chicken\", \"burger\"],\n",
    "    [\"doctor\", \"office\", \"medical\"]\n",
    "]\n",
    "\n",
    "seed_topics = {}\n",
    "for topic_id, topic_words in enumerate(seed_topic_list):\n",
    "    for word in topic_words:\n",
    "        seed_topics[vocab_dict[word]] = topic_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = guidedlda.GuidedLDA(n_topics=N_TOPIC, n_iter=N_ITER, random_state=RANDOM_STATE, refresh=REFRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 10000\n",
      "INFO:guidedlda:vocab_size: 3266\n",
      "INFO:guidedlda:n_words: 581744\n",
      "INFO:guidedlda:n_topics: 25\n",
      "INFO:guidedlda:n_iter: 500\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -7147556\n",
      "INFO:guidedlda:<20> log likelihood: -4801045\n",
      "INFO:guidedlda:<40> log likelihood: -4696974\n",
      "INFO:guidedlda:<60> log likelihood: -4644911\n",
      "INFO:guidedlda:<80> log likelihood: -4607643\n",
      "INFO:guidedlda:<100> log likelihood: -4583004\n",
      "INFO:guidedlda:<120> log likelihood: -4565122\n",
      "INFO:guidedlda:<140> log likelihood: -4552248\n",
      "INFO:guidedlda:<160> log likelihood: -4540391\n",
      "INFO:guidedlda:<180> log likelihood: -4530396\n",
      "INFO:guidedlda:<200> log likelihood: -4523068\n",
      "INFO:guidedlda:<220> log likelihood: -4519006\n",
      "INFO:guidedlda:<240> log likelihood: -4512365\n",
      "INFO:guidedlda:<260> log likelihood: -4507948\n",
      "INFO:guidedlda:<280> log likelihood: -4503770\n",
      "INFO:guidedlda:<300> log likelihood: -4499839\n",
      "INFO:guidedlda:<320> log likelihood: -4497861\n",
      "INFO:guidedlda:<340> log likelihood: -4495275\n",
      "INFO:guidedlda:<360> log likelihood: -4493083\n",
      "INFO:guidedlda:<380> log likelihood: -4490930\n",
      "INFO:guidedlda:<400> log likelihood: -4488182\n",
      "INFO:guidedlda:<420> log likelihood: -4487224\n",
      "INFO:guidedlda:<440> log likelihood: -4485301\n",
      "INFO:guidedlda:<460> log likelihood: -4483581\n",
      "INFO:guidedlda:<480> log likelihood: -4483215\n",
      "INFO:guidedlda:<499> log likelihood: -4481240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x615822980>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(dtm, seed_topics=seed_topics, seed_confidence=SEED_CONFIDENCE)\n",
    "# model.fit(dtm, seed_confidence=SEED_CONFIDENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Topic 0: good burger food fry place like chicken sandwich ordered cheese really ni back would got time meat one get go sauce try also nthe menu\n",
      "Topic 1: back said would time get told one customer go went never service could got minute hour asked even called call like know ni another come\n",
      "Topic 2: time place service great always good ha staff go like love year one location best work every never recommend get really customer new ever feel\n",
      "Topic 3: nail hair salon great really get back done place time cut massage nice groupon job like go gel one would polish appointment went service manicure\n",
      "Topic 4: chicken food good rice dish thai sauce soup noodle ordered like chinese place fried roll spicy beef restaurant curry shrimp flavor pho order pork nthe\n",
      "Topic 5: would service told customer day never time back get call card business one review phone ha called charge experience manager could company asked ni make\n",
      "Topic 6: beer place bar good like one great food selection drink find lot ha pretty price location game get nthe go pub store area time around\n",
      "Topic 7: show vega la ticket see seat flight strip amazing well best nthe one love time get trip good view cirque fun part experience nif worth\n",
      "Topic 8: food place service like good bad go would one time better star restaurant get ever much eat worst really even never terrible back nothing horrible\n",
      "Topic 9: sushi buffet food roll good place price eat vega quality like go really better get crab pretty fish service selection would best restaurant great fresh\n",
      "Topic 10: breakfast egg food brunch good bacon pancake toast french coffee waffle sausage morning get hash really omelet nthe service bagel wait brown fruit potato great\n",
      "Topic 11: class place like kid time staff gym one nice get great room day also people every lot ha clean really area feel go hour facility\n",
      "Topic 12: good cheese salad also menu delicious dish sauce dessert nthe restaurant meal cream like great flavor bread made ordered would really potato amazing sweet fresh\n",
      "Topic 13: food great place service good time love friendly always back restaurant lunch go menu delicious best staff definitely amazing happy atmosphere excellent try get favorite\n",
      "Topic 14: pizza good cheese like crust sauce salad slice place bread italian pasta get order time sandwich got topping ordered deli garlic really fresh little better\n",
      "Topic 15: ordered steak food good meal came restaurant waiter time service table order server one got dinner back drink salad would took asked cooked minute well\n",
      "Topic 16: dog parking car place vega park lot get strip location hot area street great small la die ha da inside mile near friendly located outside\n",
      "Topic 17: place drink get bar like people night time go one good really club music got friend back great pretty around cool nice vega nthe fun\n",
      "Topic 18: store like one time place go shop really price find good ha year get even also item great something much new ni need everything around\n",
      "Topic 19: room hotel stay night casino nice bed pool stayed floor vega strip like would nthe bathroom desk check free clean one staying suite front great\n",
      "Topic 20: taco salsa mexican de chip le bean la burrito u00e9 et enchilada rice est tortilla guacamole margarita u00e0 un que asada carne pa plus good\n",
      "Topic 21: like place get one much cupcake think thing go review cake ni know little yelp say make good ice made want cream com something day\n",
      "Topic 22: food great restaurant place good nice wine bar really star service table nthe price experience menu would ni like atmosphere much back little one bit\n",
      "Topic 23: food table minute time order service server drink one back get came would asked took restaurant ordered wait even never waited went got another waitress\n",
      "Topic 24: place coffee good get food like one tea love always great time order go make really ha drink also little friendly nice pretty flavor sandwich\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "print(len(topic_word))\n",
    "n_top_words = 25\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # print(topic_dist)\n",
    "    topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Topic given Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   7,   14,   26,   55,   58,   66,   86,  143,  149,  167,  176,\n",
      "        177,  184,  190,  210,  219,  240,  242,  279,  301,  311,  320,\n",
      "        321,  329,  331,  333,  339,  341,  376,  382,  388,  395,  401,\n",
      "        404,  414,  428,  429,  456,  511,  512,  516,  527,  571,  572,\n",
      "        580,  590,  609,  614,  631,  648,  661,  683,  689,  691,  697,\n",
      "        700,  721,  736,  760,  783,  787,  798,  801,  834,  872,  883,\n",
      "        909,  957,  979,  988, 1002, 1019, 1030, 1096, 1102, 1105, 1124,\n",
      "       1143, 1155, 1156, 1158, 1159, 1185, 1192, 1210, 1219, 1236, 1243,\n",
      "       1250, 1253, 1255, 1259, 1272, 1289, 1291, 1303, 1310, 1319, 1323,\n",
      "       1350, 1358, 1359, 1381, 1411, 1418, 1439, 1489, 1495, 1497, 1515,\n",
      "       1522, 1555, 1614, 1636, 1638, 1641, 1652, 1745, 1831, 1838, 1856,\n",
      "       1861, 1865, 1882, 1896, 1897, 1927, 1931, 1945, 1946, 1962, 1968,\n",
      "       1969, 1977, 1989, 2000, 2006, 2015, 2034, 2035, 2056, 2068, 2090,\n",
      "       2092, 2105, 2114, 2118, 2140, 2154, 2177, 2188, 2199, 2216, 2222,\n",
      "       2228, 2230, 2235, 2236, 2258, 2299, 2303, 2308, 2323, 2333, 2337,\n",
      "       2352, 2380, 2417, 2456, 2482, 2497, 2508, 2509, 2556, 2577, 2594,\n",
      "       2621, 2622, 2623, 2626, 2649, 2656, 2663, 2687, 2689, 2698, 2726,\n",
      "       2760, 2764, 2768, 2774, 2796, 2811, 2816, 2829, 2833, 2843, 2853,\n",
      "       2856, 2867, 2882, 2896, 2920, 2925, 2933, 2934, 2991, 2999, 3013,\n",
      "       3024, 3033, 3036, 3048, 3064, 3090, 3109, 3110, 3125, 3126, 3144,\n",
      "       3147, 3171, 3172, 3175, 3257, 3276, 3280, 3331, 3339, 3341, 3347,\n",
      "       3355, 3376, 3435, 3459, 3460, 3472, 3551, 3576, 3607, 3608, 3625,\n",
      "       3688, 3705, 3707, 3710, 3733, 3737, 3743, 3744, 3746, 3762, 3780,\n",
      "       3781, 3796, 3839, 3879, 3889, 3890, 3915, 3935, 3937, 3944, 3957,\n",
      "       3971, 3997, 4008, 4017, 4031, 4039, 4048, 4052, 4074, 4075, 4089,\n",
      "       4097, 4121, 4127, 4130, 4167, 4173, 4185, 4188, 4196, 4211, 4234,\n",
      "       4248, 4275, 4284, 4305, 4329, 4335, 4379, 4380, 4383, 4435, 4444,\n",
      "       4460, 4463, 4514, 4520, 4523, 4558, 4567, 4586, 4599, 4606, 4616,\n",
      "       4621, 4640, 4651, 4663, 4676, 4712, 4720, 4760, 4764, 4771, 4819,\n",
      "       4835, 4873, 4880, 4881, 4903, 4919, 4923, 4925, 4929, 4931, 4941,\n",
      "       4970, 4996, 5008, 5045, 5050, 5084, 5108, 5127, 5178, 5201, 5205,\n",
      "       5212, 5246, 5261, 5267, 5301, 5343, 5401, 5415, 5419, 5427, 5438,\n",
      "       5460, 5464, 5473, 5476, 5484, 5516, 5528, 5529, 5533, 5556, 5559,\n",
      "       5589, 5592, 5596, 5602, 5616, 5634, 5655, 5660, 5666, 5694, 5700,\n",
      "       5718, 5739, 5751, 5759, 5779, 5784, 5796, 5811, 5838, 5850, 5912,\n",
      "       5925, 5935, 5948, 5961, 5970, 5977, 6018, 6026, 6053, 6061, 6063,\n",
      "       6067, 6077, 6097, 6129, 6140, 6143, 6188, 6208, 6218, 6230, 6249,\n",
      "       6263, 6278, 6309, 6312, 6315, 6321, 6347, 6351, 6360, 6361, 6416,\n",
      "       6427, 6430, 6431, 6444, 6446, 6453, 6463, 6465, 6499, 6539, 6562,\n",
      "       6566, 6603, 6615, 6673, 6690, 6691, 6711, 6721, 6725, 6734, 6750,\n",
      "       6783, 6806, 6809, 6814, 6825, 6840, 6842, 6846, 6849, 6876, 6892,\n",
      "       6916, 6955, 6969, 6974, 6992, 7020, 7027, 7047, 7073, 7086, 7093,\n",
      "       7099, 7134, 7141, 7152, 7165, 7184, 7214, 7226, 7258, 7262, 7339,\n",
      "       7350, 7353, 7354, 7357, 7365, 7415, 7418, 7431, 7434, 7471, 7473,\n",
      "       7479, 7495, 7531, 7539, 7543, 7544, 7604, 7608, 7610, 7615, 7635,\n",
      "       7678, 7682, 7688, 7708, 7760, 7762, 7767, 7768, 7783, 7790, 7797,\n",
      "       7812, 7813, 7814, 7816, 7852, 7861, 7903, 7915, 7922, 7923, 7936,\n",
      "       7959, 7962, 7966, 7991, 7998, 8007, 8039, 8083, 8096, 8111, 8124,\n",
      "       8138, 8179, 8204, 8205, 8233, 8237, 8257, 8267, 8272, 8310, 8369,\n",
      "       8379, 8387, 8406, 8413, 8454, 8463, 8480, 8500, 8505, 8528, 8535,\n",
      "       8590, 8596, 8630, 8647, 8654, 8674, 8690, 8714, 8741, 8746, 8749,\n",
      "       8772, 8778, 8780, 8786, 8789, 8797, 8802, 8805, 8822, 8825, 8862,\n",
      "       8866, 8898, 8925, 8927, 8949, 8962, 8967, 8968, 8969, 9011, 9035,\n",
      "       9042, 9048, 9049, 9056, 9064, 9077, 9081, 9110, 9128, 9138, 9186,\n",
      "       9261, 9265, 9268, 9328, 9329, 9377, 9385, 9416, 9419, 9492, 9496,\n",
      "       9497, 9535, 9652, 9664, 9697, 9705, 9750, 9764, 9783, 9791, 9795,\n",
      "       9810, 9824, 9830, 9832, 9879, 9882, 9887, 9892, 9894, 9895, 9954,\n",
      "       9967, 9969, 9972, 9980]),)\n"
     ]
    }
   ],
   "source": [
    "dtm_transform = model.transform(dtm)\n",
    "\n",
    "dtm_topics = np.argmax(dtm_transform, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 14 26 55 58]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[ 220  395  399  407  406  423  421  110  396  412  400  415  411  413\n",
      "   414]\n",
      " [ 372    1  142  683  140   28  685  392  684  119  678  199  682  681\n",
      "    89]\n",
      " [  52  411  963  904  409  966  603  962  964  965  967  266  968  969\n",
      "   902]\n",
      " [ 142  372   89  834   52  397 1408  362  268   29  202   98  413 1193\n",
      "   574]\n",
      " [ 220   52  834  372   92  351   28  143 1430  912   27 1244 1295 1180\n",
      "  1432]]\n",
      "[['good' 'beer' 'brunch' 'flatbread' 'dog' 'patio' 'opted' 'my' 'biscuit'\n",
      "  'gravy' 'calling' 'grilled' 'fry' 'great' 'greatest']\n",
      " ['pizza' 'all' 'we' 'own' 'very' 'on' 'saw' 'than' 'premium' 'one'\n",
      "  'agreed' 'ever' 'mixed' 'ingredient' 'had']\n",
      " ['with' 'fry' 'lunch' 'sub' 'french' 'reasonable' 'nothing' 'free'\n",
      "  'mention' 'presented' 'recently' 'or' 'soda' 'turkey' 'run']\n",
      " ['we' 'pizza' 'had' 'crust' 'with' 'bread' 'delivered' 'cheese'\n",
      "  'ordered' 'only' 'extra' 'into' 'great' 'recommended' 'square']\n",
      " ['good' 'with' 'crust' 'pizza' 'here' 'still' 'on' 'weekend' 'general'\n",
      "  'during' 'ny' 'various' 'slice' 'isn' 'spectacular']]\n",
      "good beer brunch flatbread dog patio opted my biscuit gravy calling grilled fry great greatest\n",
      "pizza all we own very on saw than premium one agreed ever mixed ingredient had\n",
      "with fry lunch sub french reasonable nothing free mention presented recently or soda turkey run\n",
      "we pizza had crust with bread delivered cheese ordered only extra into great recommended square\n",
      "good with crust pizza here still on weekend general during ny various slice isn spectacular\n"
     ]
    }
   ],
   "source": [
    "vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "for topic_idx in range(N_TOPIC):\n",
    "    # Get 5 example documents\n",
    "    topic_idx_docs = np.where(dtm_topics == topic_idx)[0][:5]\n",
    "    print(topic_idx_docs)\n",
    "    doc_dtm = dtm[topic_idx_docs].toarray()\n",
    "    print(doc_dtm)\n",
    "    # Get 15 terms from each document\n",
    "    common_terms = np.argsort(doc_dtm, axis=1)[:, ::-1][:,:15]\n",
    "    print(common_terms)\n",
    "    print(vocab_map(common_terms))\n",
    "    document_words = vocab_map(common_terms)\n",
    "    for doc in document_words:\n",
    "        print(\" \".join(doc))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x31595 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 75714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 1000 reviews\n",
    "item_term_matrix, _ = create_matrix(processed_corpus[:1000])\n",
    "item_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 20  6 20  0  8  8 24  1 24 14 24  6  6  2  8 24 19  1 19 19 19 19 23\n",
      " 13  6 17 18 20 20  1  0  3  4 17 15 15 15 15  2 15 17 18  9 13 15  0 15\n",
      " 20  1]\n",
      "[ 4 31 46]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[  323   332   169 ... 21069 21070 15797]\n",
      " [ 1666   554   190 ... 21069 21070     0]\n",
      " [ 1137   243   612 ... 21069 21070     0]]\n",
      "[[ 323  332  169  338   70]\n",
      " [1666  554  190  171  586]\n",
      " [1137  243  612  169 2057]]\n",
      "[['hot' 'sauce' 'are' 'wing' 'good']\n",
      " ['chinese' 'food' 'even' 'away' 'taste']\n",
      " ['soup' 'best' 'around' 'are' 'chow']]\n",
      "Top topic: 20, Document terms: he, you, him, need, really, have, see, without, parent, referral, patient, on, patronizing, offer, practitioner\n",
      "Top topic: 20, Document terms: he, all, very, been, over, your, dr, patient, year, out, think, health, option, question, one\n",
      "Top topic: 6, Document terms: you, office, he, doctor, not, when, about, away, they, me, call, johnson, will, before, practice\n",
      "Top topic: 20, Document terms: have, dr, doctor, his, very, we, him, many, who, come, been, ha, diagnosed, every, had\n",
      "Top topic: 0, Document terms: hot, sauce, are, wing, good, fish, extra, flavor, frank, red, heat, decent, large, lot, maybe\n",
      "Top topic: 8, Document terms: out, ball, range, are, driving, place, they, you, money, only, sell, light, make, let, mat\n",
      "Top topic: 8, Document terms: look, wait, range, can, on, tee, state, turf, be, yesterday, amazing, nice, ll, really, re\n",
      "Top topic: 24, Document terms: sandwich, were, reuben, fish, on, we, had, they, very, pretty, time, giant, be, extremely, recommendation\n",
      "Top topic: 1, Document terms: you, are, all, place, on, food, re, don, if, good, they, one, get, nthe, lot\n",
      "Top topic: 24, Document terms: good, fish, sandwich, lobsicle, spain, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced\n",
      "Top topic: 14, Document terms: mushroom, an, from, canned, place, they, ever, tasty, know, crowd, were, pizza, staff, bar, night\n",
      "Top topic: 24, Document terms: coleslaw, we, kraut, very, they, charged, be, disappointed, instead, customer, tasting, back, reuben, not, iced\n",
      "Top topic: 6, Document terms: tire, wait, get, they, toyota, there, price, hour, new, good, long, week, lost, make, looked\n",
      "Top topic: 6, Document terms: had, they, tire, called, them, overpriced, seem, summer, super, on, didn, going, also, time, very\n",
      "Top topic: 2, Document terms: lobsicle, sauv, unwanted, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced, prawn, spain\n",
      "Top topic: 8, Document terms: really, don, found, associate, luckily, mill, puppy, teen, treat, on, moved, am, around, young, they\n",
      "Top topic: 24, Document terms: fish, home, good, their, ordered, served, friend, rocky, fry, ate, meatball, brought, he, style, food\n",
      "Top topic: 19, Document terms: eat, definition, sausage, very, fry, here, you, unpretentious, put, thank, country, joint, favorite, home, diner\n",
      "Top topic: 1, Document terms: line, great, ready, stand, people, bad, place, be, meal, never, love, had, portion, good, innovation\n",
      "Top topic: 19, Document terms: you, are, best, place, they, make, healthy, on, grill, breakfast, like, diner, we, many, yourself\n",
      "Top topic: 19, Document terms: they, had, good, made, now, return, toast, what, eat, great, fresh, french, turkey, grill, wait\n",
      "Top topic: 19, Document terms: service, friendly, poached, only, what, eat, crispy, bacon, felt, south, family, frankly, homefries, nfantastic, gab\n",
      "Top topic: 19, Document terms: had, place, nthe, give, really, were, some, decent, price, food, country, which, good, on, service\n",
      "Top topic: 23, Document terms: people, tonya, are, front, very, sweet, desk, helpful, super, prawn, barn, brioni, 5k, measly, influenced\n",
      "Top topic: 13, Document terms: irish, again, bar, great, food, best, not, doe, better, than, place, town, what, nor, service\n",
      "Top topic: 6, Document terms: than, order, phone, his, on, other, he, would, rather, driver, town, just, or, speed, call\n",
      "Top topic: 17, Document terms: were, from, pizza, if, so, ordered, about, there, them, put, no, on, sicilian, bad, meat\n",
      "Top topic: 18, Document terms: you, stuff, are, fried, review, after, really, regret, finish, eating, ljs, cardboard, minute, boat, extra\n",
      "Top topic: 20, Document terms: her, have, had, not, all, them, she, too, tooth, an, because, on, me, care, follow\n",
      "Top topic: 20, Document terms: me, at, review, did, office, door, not, nice, distracted, very, where, there, you, dentist, waited\n",
      "Top topic: 1, Document terms: restaurant, there, favorite, don, food, eat, average, few, myself, carnegie, eating, place, think, service, had\n",
      "Top topic: 0, Document terms: chinese, food, even, away, taste, style, doesn, from, far, real, good, american, ballet, 5k, barn\n",
      "Top topic: 3, Document terms: we, minute, not, meal, came, food, our, an, order, drink, or, so, place, at, waited\n",
      "Top topic: 4, Document terms: cardio, only, you, equipment, gym, room, me, weight, us, training, session, section, personal, join, increase\n",
      "Top topic: 17, Document terms: gym, get, working, egg, try, she, by, if, instead, nasty, most, morning, complain, you, worker\n",
      "Top topic: 15, Document terms: had, time, experience, bread, sandwich, mixed, papa, first, girlfriend, take, ve, server, me, fish, care\n",
      "Top topic: 15, Document terms: white, pizza, bar, order, calamari, go, on, visit, note, nlike, our, ever, ok, style, very\n",
      "Top topic: 15, Document terms: nthe, had, pizza, calamari, white, we, take, time, garlic, very, order, style, some, fried, than\n",
      "Top topic: 15, Document terms: food, everyone, serving, yummy, great, dessert, be, gem, little, place, very, try, without, you, price\n",
      "Top topic: 2, Document terms: liked, place, or, down, great, because, be, california, imagine, relocating, crazy, street, stroll, visitor, couple\n",
      "Top topic: 15, Document terms: restaurant, dish, had, were, on, ordered, made, at, zucchini, not, chicken, marsala, which, she, too\n",
      "Top topic: 17, Document terms: terrible, like, wood, smell, place, wet, rotten, service, food, barn, brioni, lobsicle, prawn, 5k, measly\n",
      "Top topic: 18, Document terms: justify, inconsistent, there, expensive, nice, carnegie, restaurant, price, better, ambiance, are, papa, doesn, spain, innovation\n",
      "Top topic: 9, Document terms: had, we, manager, service, minute, want, you, said, back, get, go, bill, do, our, your\n",
      "Top topic: 13, Document terms: always, place, fresh, so, love, well, food, friendly, staff, delicious, douche, prawn, corresponding, nsuper, caterer\n",
      "Top topic: 15, Document terms: had, back, great, size, on, really, pepper, would, very, what, enough, good, green, just, sauce\n",
      "Top topic: 0, Document terms: soup, best, around, are, chow, salad, some, place, great, bread, love, 5k, innovation, measly, brioni\n",
      "Top topic: 15, Document terms: at, pasta, had, seriously, we, service, went, could, rather, down, hill, price, eat, she, wine\n",
      "Top topic: 20, Document terms: they, have, will, going, nice, be, behind, them, are, stand, re, rude, ve, unprofessional, what\n",
      "Top topic: 1, Document terms: dental, been, you, office, should, time, no, beat, worst, avoid, at, ever, can, any, one\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(item_term_matrix)\n",
    "\n",
    "top_topic = np.argmax(doc_topic, axis=1) #find the topic of the document\n",
    "print(top_topic)\n",
    "\n",
    "for topic_idx in range(N_TOPIC):\n",
    "    # Grab 5 example documents from each topic\n",
    "    topic_idx_docs = np.where(top_topic == topic_idx)[:5]\n",
    "    doc_dtm = item_term_matrix[topic_idx_docs].toarray()\n",
    "    \n",
    "\n",
    "doc_topics = [np.where(top_topic == topic_idx) for topic_idx in N_TOPIC] # a list of list where the [i][.] corresponds to list of document in topic i\n",
    "topic_doc_indices = np.where(top_topic == 0)[0][:5] #find documents from topic 0\n",
    "print(topic_doc_indices)\n",
    "print(item_term_matrix[topic_doc_indices].toarray()) #print document term matrix for those documents\n",
    "print(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1]) #argsort to find the most common terms\n",
    "print(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1][:, :5]) #same thing, gives top 5\n",
    "\n",
    "vectorized_vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "print(vectorized_vocab_map(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1][:, :5])) #get words corresponding to indices\n",
    "\n",
    "for i in range(len(doc_topic)):  # Use the correct range based on your data\n",
    "    # Get the top topic for the document\n",
    "    top_topic = doc_topic[i].argmax()\n",
    "    \n",
    "    # Get the top terms more frequent than 1 in the document from the dtm\n",
    "    row = item_term_matrix[i, :].toarray().flatten()  # Convert sparse row to dense\n",
    "    # item_count = np.count_nonzero(row > 1)\n",
    "    top_terms_indices = row.argsort()[::-1][:15]  # Indices of the top 10 terms\n",
    "    top_terms = [vocab_list[idx] for idx in top_terms_indices]\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Top topic: {top_topic}, Document terms: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dataset = []\n",
    "topic_review = [] # reviews in topics \n",
    "\n",
    "output_review_num = 5 # output the top 5 reviwew from each topic\n",
    "\n",
    "for topic in range(N_TOPIC):\n",
    "    if len(doc_topics[topic]) >= 5:\n",
    "        end = 5\n",
    "    else:\n",
    "        end = len(doc_topics[topic])\n",
    "    for review_idx in doc_topics[topic][:end]:\n",
    "        topic_review[topic].append(og_dataset[review_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try making code to extract 5 documents per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prunes matrices, saves weights but locks model\n",
    "# model.purge_extra_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = f\"model_{N_TOPIC}_{N_ITER}_{RANDOM_STATE}_{REFRESH}_{SEED_CONFIDENCE}.pkl\"\n",
    "\n",
    "# with open(f\"results/{modelname}\", 'wb') as file:\n",
    "    # pkl.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.23365735e-04, 1.55314518e-03, 4.57218647e-04, 2.87699878e-04,\n",
       "        1.87135438e-03, 2.22783475e-04, 2.12591695e-01, 2.84587882e-04,\n",
       "        5.12376868e-02, 5.04084413e-04, 1.43815767e-03, 3.04138392e-04,\n",
       "        3.15682987e-06, 3.29554255e-02, 1.25188044e-02, 3.53338186e-04,\n",
       "        6.78863908e-06, 3.08103614e-04, 4.92289652e-03, 4.52324088e-04,\n",
       "        6.72737138e-01, 2.55559447e-03, 1.49780723e-03, 3.40117465e-04,\n",
       "        2.72587228e-04]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the model can be loaded/works\n",
    "# with open('guidedlda_model.pickle', 'rb') as file:\n",
    "    # model = pkl.load(file)\n",
    "# model.transform(create_matrix(processed_corpus[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-768290932.3669106,\n",
       " -544845646.4478563,\n",
       " -523167504.4543283,\n",
       " -517464505.4454495,\n",
       " -515242153.73157394,\n",
       " -514171247.29473853,\n",
       " -513573261.2712008,\n",
       " -513116435.8723612]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loglikelihoods_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "\n",
    "Make sure to run this after each experiment (after you've created/fitted the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in result_25_500_7_20_0.15.txt\n"
     ]
    }
   ],
   "source": [
    "filename = f\"result_{N_TOPIC}_{N_ITER}_{RANDOM_STATE}_{REFRESH}_{SEED_CONFIDENCE}.txt\"\n",
    "\n",
    "with open(f\"results/{filename}\", \"w\") as f:\n",
    "    if seed_topic_list:\n",
    "        print(f\"{seed_topic_list}\\n\", file=f)\n",
    "    else:\n",
    "        print(\"No Seeds\\n\", file=f)\n",
    "        \n",
    "    dtm_transform = model.transform(dtm)\n",
    "    dtm_topics = np.argmax(dtm_transform, axis=1)\n",
    "    vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "\n",
    "    # Save top 25 words per topic\n",
    "    topic_word = model.topic_word_\n",
    "    n_top_words = 25\n",
    "    for idx, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(idx, ' '.join(topic_words)), file=f)\n",
    "        print(\"Example Documents:\", file=f)\n",
    "        \n",
    "        # Get 5 example documents\n",
    "        topic_idx_docs = np.where(dtm_topics == idx)[0][:5]\n",
    "        \n",
    "        doc_dtm = dtm[topic_idx_docs].toarray()\n",
    "        \n",
    "        # Get 15 terms from each document\n",
    "        common_terms = np.argsort(doc_dtm, axis=1)[:, ::-1][:,:15]\n",
    "        document_words = vocab_map(common_terms)\n",
    "        for doc in document_words:\n",
    "            print(\" \".join(doc), file=f)\n",
    "        \n",
    "        # Newline as separator\n",
    "        print(file=f)\n",
    "    \n",
    "\n",
    "print(\"Saved in\", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragnarec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
