{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'goldberg', 'offer', 'everything', 'look', 'for', 'in', 'general', 'practitioner', 'he', 'nice', 'and', 'easy', 'to', 'talk', 'to', 'without', 'being', 'patronizing', 'he', 'always', 'on', 'time', 'in', 'seeing', 'his', 'patient', 'he', 'affiliated', 'with', 'top', 'notch', 'hospital', 'nyu', 'which', 'my', 'parent', 'have', 'explained', 'to', 'me', 'is', 'very', 'important', 'in', 'case', 'something', 'happens', 'and', 'you', 'need', 'surgery', 'and', 'you', 'can', 'get', 'referral', 'to', 'see', 'specialist', 'without', 'having', 'to', 'see', 'him', 'first', 'really', 'what', 'more', 'do', 'you', 'need', 'sitting', 'here', 'trying', 'to', 'think', 'of', 'any', 'complaint', 'have', 'about', 'him', 'but', 'really', 'drawing', 'blank']\n"
     ]
    }
   ],
   "source": [
    "# get the clearned reviews \n",
    "with open(\"clean_reviews.pkl\", \"rb\") as file:\n",
    "    clean_reviews = pkl.load(file)\n",
    "    print(clean_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 3)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"clean_review_corpus.pkl\", \"rb\") as file:\n",
    "    corpus = pkl.load(file)\n",
    "    print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab_list\n",
    "dictionary = Dictionary(clean_reviews)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "vocab_list = list(dictionary.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.pkl\", \"wb\") as file:\n",
    "    pkl.dump(vocab_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(processed_text, vocabulary=vocab_list):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    dtm = vectorizer.fit_transform(processed_text)\n",
    "    return dtm, vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the seed topics in dict and in matrix\n",
    "# seed_topics = {\n",
    "#     \"doctor\": [\"medicine\", \"office\", \"hurt\"],\n",
    "#     \"food\": [\"coffee\", \"pizza\", \"delicious\"]\n",
    "# }   \n",
    "\n",
    "# processed_seeds = [\" \".join(topic) for topic in list(seed_topics.values())]\n",
    "\n",
    "# seed_topic_list, vocab_dict = create_matrix(processed_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = [\" \".join(tokens) for tokens in clean_reviews]\n",
    "dtm, vocab_dict = create_matrix(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list = [[\"medicine\", \"office\", \"hurt\"],[\"coffee\", \"pizza\", \"delicious\"]]\n",
    "\n",
    "seed_topics = {}\n",
    "for topic_id, topic_words in enumerate(seed_topic_list):\n",
    "    for word in topic_words:\n",
    "        seed_topics[vocab_dict[word]] = topic_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = guidedlda.GuidedLDA(n_topics=2, n_iter=10, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 700000\n",
      "INFO:guidedlda:vocab_size: 31595\n",
      "INFO:guidedlda:n_words: 65988636\n",
      "INFO:guidedlda:n_topics: 2\n",
      "INFO:guidedlda:n_iter: 10\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -535905597\n",
      "INFO:guidedlda:<9> log likelihood: -521357147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x15ac6fd90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(dtm, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: you we they on not have at had\n",
      "Topic 1: we you they on not have had were\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc_topic \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop topic: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Document: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(doc_topic[i]\u001b[38;5;241m.\u001b[39margmax(),\n\u001b[1;32m      4\u001b[0m                                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(np\u001b[38;5;241m.\u001b[39marray(vocab_list)[\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(dtm[i,:]\u001b[38;5;241m.\u001b[39margsort()))[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m]])))\n",
      "File \u001b[0;32m/Users/CalvQ/git/GuidedLDA/guidedlda/guidedlda.py:188\u001b[0m, in \u001b[0;36mGuidedLDA.transform\u001b[0;34m(self, X, max_iter, tol)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# TODO: this loop is parallelizable\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(DS):\n\u001b[0;32m--> 188\u001b[0m     doc_topic[d] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc_topic\n",
      "File \u001b[0;32m/Users/CalvQ/git/GuidedLDA/guidedlda/guidedlda.py:191\u001b[0m, in \u001b[0;36mGuidedLDA._transform_single\u001b[0;34m(self, doc, max_iter, tol)\u001b[0m\n\u001b[1;32m    188\u001b[0m         doc_topic[d] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_single(WS[DS \u001b[38;5;241m==\u001b[39m d], max_iter, tol)\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_topic\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, max_iter, tol):\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform a single document according to the previously fit model\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     PZS \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(doc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_topics))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(dtm)\n",
    "for i in range(9):\n",
    "    print(\"top topic: {} Document: {}\".format(doc_topic[i].argmax(),\n",
    "                                                  ', '.join(np.array(vocab_list)[list(reversed(dtm[i,:].argsort()))[0:5]])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10701-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
