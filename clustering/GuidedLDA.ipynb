{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'goldberg', 'offer', 'everything', 'look', 'for', 'in', 'general', 'practitioner', 'he', 'nice', 'and', 'easy', 'to', 'talk', 'to', 'without', 'being', 'patronizing', 'he', 'always', 'on', 'time', 'in', 'seeing', 'his', 'patient', 'he', 'affiliated', 'with', 'top', 'notch', 'hospital', 'nyu', 'which', 'my', 'parent', 'have', 'explained', 'to', 'me', 'is', 'very', 'important', 'in', 'case', 'something', 'happens', 'and', 'you', 'need', 'surgery', 'and', 'you', 'can', 'get', 'referral', 'to', 'see', 'specialist', 'without', 'having', 'to', 'see', 'him', 'first', 'really', 'what', 'more', 'do', 'you', 'need', 'sitting', 'here', 'trying', 'to', 'think', 'of', 'any', 'complaint', 'have', 'about', 'him', 'but', 'really', 'drawing', 'blank']\n"
     ]
    }
   ],
   "source": [
    "# get the clearned reviews \n",
    "with open(\"clean_reviews.pkl\", \"rb\") as file:\n",
    "    clean_reviews = pkl.load(file)\n",
    "    print(clean_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 3)]\n"
     ]
    }
   ],
   "source": [
    "# with open(\"clean_review_corpus.pkl\", \"rb\") as file:\n",
    "#     corpus = pkl.load(file)\n",
    "#     print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsample Reviews for Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_reviews))\n",
    "random.seed(10701)\n",
    "clean_reviews_down = random.sample(clean_reviews, 10000)\n",
    "print(len(clean_reviews_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'wa',\n",
       " 'the',\n",
       " 'first',\n",
       " 'place',\n",
       " 'in',\n",
       " 'vega',\n",
       " 'where',\n",
       " 'the',\n",
       " 'yelpers',\n",
       " 'let',\n",
       " 'me',\n",
       " 'down',\n",
       " 'the',\n",
       " 'salt',\n",
       " 'and',\n",
       " 'pepper',\n",
       " 'shrimp',\n",
       " 'appetizer',\n",
       " 'wa',\n",
       " 'inedible',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'shell',\n",
       " 'wa',\n",
       " 'left',\n",
       " 'on',\n",
       " 'underneath',\n",
       " 'the',\n",
       " 'crispy',\n",
       " 'fried',\n",
       " 'coating',\n",
       " 'a',\n",
       " 'result',\n",
       " 'the',\n",
       " 'sauce',\n",
       " 'used',\n",
       " 'to',\n",
       " 'marinate',\n",
       " 'the',\n",
       " 'shrimp',\n",
       " 'never',\n",
       " 'penetrated',\n",
       " 'the',\n",
       " 'meat',\n",
       " 'it',\n",
       " 'coagulated',\n",
       " 'in',\n",
       " 'the',\n",
       " 'head',\n",
       " 'and',\n",
       " 'exploded',\n",
       " 'on',\n",
       " 'you',\n",
       " 'upon',\n",
       " 'taking',\n",
       " 'bite',\n",
       " 'gross',\n",
       " 'the',\n",
       " 'potstickers',\n",
       " 'and',\n",
       " 'the',\n",
       " 'beef',\n",
       " 'chow',\n",
       " 'fun',\n",
       " 'lacked',\n",
       " 'any',\n",
       " 'flavor',\n",
       " 'whatsoever',\n",
       " 'the',\n",
       " 'only',\n",
       " 'redeeming',\n",
       " 'quality',\n",
       " 'the',\n",
       " 'service',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'awful',\n",
       " 'is',\n",
       " 'the',\n",
       " 'roast',\n",
       " 'duck',\n",
       " 'crispy',\n",
       " 'and',\n",
       " 'tasty',\n",
       " 'skin',\n",
       " 'with',\n",
       " 'relatively',\n",
       " 'moist',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'relative',\n",
       " 'bargain',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'place',\n",
       " 'in',\n",
       " 'ny',\n",
       " 'and']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews_down[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<28589 unique tokens: ['a', 'all', 'and', 'any', 'appetizer']...> from 10000 documents (total 1251924 corpus positions)\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<28589 unique tokens: ['a', 'all', 'and', 'any', 'appetizer']...> from 10000 documents (total 1251924 corpus positions)\", 'datetime': '2024-11-19T11:35:47.712755', 'gensim': '4.3.3', 'python': '3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]', 'platform': 'macOS-14.7-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO:gensim.corpora.dictionary:discarding 25200 tokens: [('and', 8815), ('coagulated', 1), ('coating', 12), ('exploded', 3), ('in', 6244), ('is', 6213), ('it', 6913), ('marinate', 2), ('of', 6719), ('penetrated', 1)]...\n",
      "INFO:gensim.corpora.dictionary:keeping 3389 tokens which were in no less than 20 and no more than 5000 (=50.0%) documents\n",
      "INFO:gensim.corpora.dictionary:resulting dictionary: Dictionary<3389 unique tokens: ['a', 'all', 'any', 'appetizer', 'awful']...>\n"
     ]
    }
   ],
   "source": [
    "# get the vocab_list\n",
    "dictionary = Dictionary(clean_reviews_down)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "vocab_list = list(dictionary.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vocabulary.pkl\", \"wb\") as file:\n",
    "#     pkl.dump(vocab_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"vocabulary.pkl\", \"rb\") as file:\n",
    "#     vocab_list = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(processed_text, vocabulary=vocab_list):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    dtm = vectorizer.fit_transform(processed_text)\n",
    "    return dtm, vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = [\" \".join(tokens) for tokens in clean_reviews_down]\n",
    "dtm, vocab_dict = create_matrix(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_TOPIC = 25            # number of clusters\n",
    "N_ITER = 500            # number of iterations run on dataset\n",
    "RANDOM_STATE = 7        # initial random seed\n",
    "REFRESH = 20            # how often you print log-likelihood\n",
    "SEED_CONFIDENCE=0.15    # probability of using this word as a seed\n",
    "\n",
    "# create topic seeds\n",
    "# seed_topic_list = [\n",
    "#     [\"medicine\", \"office\", \"hurt\"],\n",
    "#     [\"coffee\", \"pizza\", \"delicious\"]\n",
    "# ]\n",
    "seed_topic_list = [\n",
    "    [\"pizza\", \"food\", \"chicken\", \"burger\"],\n",
    "    [\"doctor\", \"office\", \"medical\"]\n",
    "]\n",
    "\n",
    "seed_topics = {}\n",
    "for topic_id, topic_words in enumerate(seed_topic_list):\n",
    "    for word in topic_words:\n",
    "        seed_topics[vocab_dict[word]] = topic_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Fitting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = guidedlda.GuidedLDA(n_topics=N_TOPIC, n_iter=N_ITER, random_state=RANDOM_STATE, refresh=REFRESH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 10000\n",
      "INFO:guidedlda:vocab_size: 3389\n",
      "INFO:guidedlda:n_words: 872500\n",
      "INFO:guidedlda:n_topics: 25\n",
      "INFO:guidedlda:n_iter: 500\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -9980162\n",
      "INFO:guidedlda:<20> log likelihood: -7004175\n",
      "INFO:guidedlda:<40> log likelihood: -6845304\n",
      "INFO:guidedlda:<60> log likelihood: -6780632\n",
      "INFO:guidedlda:<80> log likelihood: -6740308\n",
      "INFO:guidedlda:<100> log likelihood: -6711140\n",
      "INFO:guidedlda:<120> log likelihood: -6686449\n",
      "INFO:guidedlda:<140> log likelihood: -6666613\n",
      "INFO:guidedlda:<160> log likelihood: -6654162\n",
      "INFO:guidedlda:<180> log likelihood: -6641791\n",
      "INFO:guidedlda:<200> log likelihood: -6631873\n",
      "INFO:guidedlda:<220> log likelihood: -6625680\n",
      "INFO:guidedlda:<240> log likelihood: -6621122\n",
      "INFO:guidedlda:<260> log likelihood: -6614256\n",
      "INFO:guidedlda:<280> log likelihood: -6608415\n",
      "INFO:guidedlda:<300> log likelihood: -6607059\n",
      "INFO:guidedlda:<320> log likelihood: -6603700\n",
      "INFO:guidedlda:<340> log likelihood: -6602095\n",
      "INFO:guidedlda:<360> log likelihood: -6596615\n",
      "INFO:guidedlda:<380> log likelihood: -6595549\n",
      "INFO:guidedlda:<400> log likelihood: -6592944\n",
      "INFO:guidedlda:<420> log likelihood: -6591454\n",
      "INFO:guidedlda:<440> log likelihood: -6587343\n",
      "INFO:guidedlda:<460> log likelihood: -6585539\n",
      "INFO:guidedlda:<480> log likelihood: -6582942\n",
      "INFO:guidedlda:<499> log likelihood: -6583770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x6158202b0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "model.fit(dtm, seed_topics=seed_topics, seed_confidence=SEED_CONFIDENCE)\n",
    "# model.fit(dtm, seed_confidence=SEED_CONFIDENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words per Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Topic 0: pizza burger good with fry my they had cheese not on sandwich place have food like chicken so just are very were all sauce salad\n",
      "Topic 1: my she me he her so not had on said just back when what like up have there with at would one asked out did\n",
      "Topic 2: room hotel at with not stay on vega are nice pool night casino very strip bed you from had stayed there here floor were which\n",
      "Topic 3: de le la u00e9 et n1 n2 est n3 u00e0 un que pa da en die n4 u00e9e on pour du une plus au service\n",
      "Topic 4: with were had we my on dessert which salad steak cream meal menu wine cheese so all delicious restaurant our good very an bread made\n",
      "Topic 5: food not had with restaurant were good dish very sauce my nthe menu salad just like be chicken on would or they star service flavor\n",
      "Topic 6: food good roll chicken rice with place sushi my soup not thai very fried had spicy noodle they chinese dish on here restaurant like sauce\n",
      "Topic 7: my they me not on with have had be at he them time an no when would will back service their up from get you\n",
      "Topic 8: store at on are from one there with shop not location have find ha price selection or item more some be shopping well lot like\n",
      "Topic 9: you they have are place if food good service time go not be can at there been great here get always or their don ha\n",
      "Topic 10: we our were food table minute service had order not she at there server drink my time after with they be have no came he\n",
      "Topic 11: they have you my store are nail with here very on place so there other at not be all get from like just dog one\n",
      "Topic 12: they not you like just so place have if good me my really had would get be don what all or go are were there\n",
      "Topic 13: breakfast with egg good on had my were they have bacon sandwich bbq so food place not very coffee like side potato pork meat chicken\n",
      "Topic 14: buffet food not at you good all price here vega line place get are better like have would quality there eat no can more they\n",
      "Topic 15: on bar with there drink were at place so night we music you people club great had like here good get have all not some\n",
      "Topic 16: we food had my good were time service here place not on be back very so our have at went out there just will great\n",
      "Topic 17: you they have are place your don there if like here get on can so go do out time not no their just even been\n",
      "Topic 18: my with have show you so me very she had be time all her great not class are on they really from like at about\n",
      "Topic 19: we were our they had on at up would time there be so have back one when out told after got with nwe hour an\n",
      "Topic 20: you they have are not if like there can or so place on get your be what my don me just about out one at\n",
      "Topic 21: were we with my had on good they not taco ordered chicken which food chip salsa so just fish our cheese very sauce bean be\n",
      "Topic 22: we our were had my not food at he out ordered so on with they came when table about order back didn meal got waiter\n",
      "Topic 23: you are place on if at your or not can be with get have so there like out some go here up re just all\n",
      "Topic 24: great place food are very have with service they my you good their love on so here friendly always best time delicious had staff coffee\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "print(len(topic_word))\n",
    "n_top_words = 25\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # print(topic_dist)\n",
    "    topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Topic given Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   7,   14,   26,   55,   58,   66,   86,  143,  149,  167,  176,\n",
      "        177,  184,  190,  210,  219,  240,  242,  279,  301,  311,  320,\n",
      "        321,  329,  331,  333,  339,  341,  376,  382,  388,  395,  401,\n",
      "        404,  414,  428,  429,  456,  511,  512,  516,  527,  571,  572,\n",
      "        580,  590,  609,  614,  631,  648,  661,  683,  689,  691,  697,\n",
      "        700,  721,  736,  760,  783,  787,  798,  801,  834,  872,  883,\n",
      "        909,  957,  979,  988, 1002, 1019, 1030, 1096, 1102, 1105, 1124,\n",
      "       1143, 1155, 1156, 1158, 1159, 1185, 1192, 1210, 1219, 1236, 1243,\n",
      "       1250, 1253, 1255, 1259, 1272, 1289, 1291, 1303, 1310, 1319, 1323,\n",
      "       1350, 1358, 1359, 1381, 1411, 1418, 1439, 1489, 1495, 1497, 1515,\n",
      "       1522, 1555, 1614, 1636, 1638, 1641, 1652, 1745, 1831, 1838, 1856,\n",
      "       1861, 1865, 1882, 1896, 1897, 1927, 1931, 1945, 1946, 1962, 1968,\n",
      "       1969, 1977, 1989, 2000, 2006, 2015, 2034, 2035, 2056, 2068, 2090,\n",
      "       2092, 2105, 2114, 2118, 2140, 2154, 2177, 2188, 2199, 2216, 2222,\n",
      "       2228, 2230, 2235, 2236, 2258, 2299, 2303, 2308, 2323, 2333, 2337,\n",
      "       2352, 2380, 2417, 2456, 2482, 2497, 2508, 2509, 2556, 2577, 2594,\n",
      "       2621, 2622, 2623, 2626, 2649, 2656, 2663, 2687, 2689, 2698, 2726,\n",
      "       2760, 2764, 2768, 2774, 2796, 2811, 2816, 2829, 2833, 2843, 2853,\n",
      "       2856, 2867, 2882, 2896, 2920, 2925, 2933, 2934, 2991, 2999, 3013,\n",
      "       3024, 3033, 3036, 3048, 3064, 3090, 3109, 3110, 3125, 3126, 3144,\n",
      "       3147, 3171, 3172, 3175, 3257, 3276, 3280, 3331, 3339, 3341, 3347,\n",
      "       3355, 3376, 3435, 3459, 3460, 3472, 3551, 3576, 3607, 3608, 3625,\n",
      "       3688, 3705, 3707, 3710, 3733, 3737, 3743, 3744, 3746, 3762, 3780,\n",
      "       3781, 3796, 3839, 3879, 3889, 3890, 3915, 3935, 3937, 3944, 3957,\n",
      "       3971, 3997, 4008, 4017, 4031, 4039, 4048, 4052, 4074, 4075, 4089,\n",
      "       4097, 4121, 4127, 4130, 4167, 4173, 4185, 4188, 4196, 4211, 4234,\n",
      "       4248, 4275, 4284, 4305, 4329, 4335, 4379, 4380, 4383, 4435, 4444,\n",
      "       4460, 4463, 4514, 4520, 4523, 4558, 4567, 4586, 4599, 4606, 4616,\n",
      "       4621, 4640, 4651, 4663, 4676, 4712, 4720, 4760, 4764, 4771, 4819,\n",
      "       4835, 4873, 4880, 4881, 4903, 4919, 4923, 4925, 4929, 4931, 4941,\n",
      "       4970, 4996, 5008, 5045, 5050, 5084, 5108, 5127, 5178, 5201, 5205,\n",
      "       5212, 5246, 5261, 5267, 5301, 5343, 5401, 5415, 5419, 5427, 5438,\n",
      "       5460, 5464, 5473, 5476, 5484, 5516, 5528, 5529, 5533, 5556, 5559,\n",
      "       5589, 5592, 5596, 5602, 5616, 5634, 5655, 5660, 5666, 5694, 5700,\n",
      "       5718, 5739, 5751, 5759, 5779, 5784, 5796, 5811, 5838, 5850, 5912,\n",
      "       5925, 5935, 5948, 5961, 5970, 5977, 6018, 6026, 6053, 6061, 6063,\n",
      "       6067, 6077, 6097, 6129, 6140, 6143, 6188, 6208, 6218, 6230, 6249,\n",
      "       6263, 6278, 6309, 6312, 6315, 6321, 6347, 6351, 6360, 6361, 6416,\n",
      "       6427, 6430, 6431, 6444, 6446, 6453, 6463, 6465, 6499, 6539, 6562,\n",
      "       6566, 6603, 6615, 6673, 6690, 6691, 6711, 6721, 6725, 6734, 6750,\n",
      "       6783, 6806, 6809, 6814, 6825, 6840, 6842, 6846, 6849, 6876, 6892,\n",
      "       6916, 6955, 6969, 6974, 6992, 7020, 7027, 7047, 7073, 7086, 7093,\n",
      "       7099, 7134, 7141, 7152, 7165, 7184, 7214, 7226, 7258, 7262, 7339,\n",
      "       7350, 7353, 7354, 7357, 7365, 7415, 7418, 7431, 7434, 7471, 7473,\n",
      "       7479, 7495, 7531, 7539, 7543, 7544, 7604, 7608, 7610, 7615, 7635,\n",
      "       7678, 7682, 7688, 7708, 7760, 7762, 7767, 7768, 7783, 7790, 7797,\n",
      "       7812, 7813, 7814, 7816, 7852, 7861, 7903, 7915, 7922, 7923, 7936,\n",
      "       7959, 7962, 7966, 7991, 7998, 8007, 8039, 8083, 8096, 8111, 8124,\n",
      "       8138, 8179, 8204, 8205, 8233, 8237, 8257, 8267, 8272, 8310, 8369,\n",
      "       8379, 8387, 8406, 8413, 8454, 8463, 8480, 8500, 8505, 8528, 8535,\n",
      "       8590, 8596, 8630, 8647, 8654, 8674, 8690, 8714, 8741, 8746, 8749,\n",
      "       8772, 8778, 8780, 8786, 8789, 8797, 8802, 8805, 8822, 8825, 8862,\n",
      "       8866, 8898, 8925, 8927, 8949, 8962, 8967, 8968, 8969, 9011, 9035,\n",
      "       9042, 9048, 9049, 9056, 9064, 9077, 9081, 9110, 9128, 9138, 9186,\n",
      "       9261, 9265, 9268, 9328, 9329, 9377, 9385, 9416, 9419, 9492, 9496,\n",
      "       9497, 9535, 9652, 9664, 9697, 9705, 9750, 9764, 9783, 9791, 9795,\n",
      "       9810, 9824, 9830, 9832, 9879, 9882, 9887, 9892, 9894, 9895, 9954,\n",
      "       9967, 9969, 9972, 9980]),)\n"
     ]
    }
   ],
   "source": [
    "dtm_transform = model.transform(dtm)\n",
    "\n",
    "dtm_topics = np.argmax(dtm_transform, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7 14 26 55 58]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[ 220  395  399  407  406  423  421  110  396  412  400  415  411  413\n",
      "   414]\n",
      " [ 372    1  142  683  140   28  685  392  684  119  678  199  682  681\n",
      "    89]\n",
      " [  52  411  963  904  409  966  603  962  964  965  967  266  968  969\n",
      "   902]\n",
      " [ 142  372   89  834   52  397 1408  362  268   29  202   98  413 1193\n",
      "   574]\n",
      " [ 220   52  834  372   92  351   28  143 1430  912   27 1244 1295 1180\n",
      "  1432]]\n",
      "[['good' 'beer' 'brunch' 'flatbread' 'dog' 'patio' 'opted' 'my' 'biscuit'\n",
      "  'gravy' 'calling' 'grilled' 'fry' 'great' 'greatest']\n",
      " ['pizza' 'all' 'we' 'own' 'very' 'on' 'saw' 'than' 'premium' 'one'\n",
      "  'agreed' 'ever' 'mixed' 'ingredient' 'had']\n",
      " ['with' 'fry' 'lunch' 'sub' 'french' 'reasonable' 'nothing' 'free'\n",
      "  'mention' 'presented' 'recently' 'or' 'soda' 'turkey' 'run']\n",
      " ['we' 'pizza' 'had' 'crust' 'with' 'bread' 'delivered' 'cheese'\n",
      "  'ordered' 'only' 'extra' 'into' 'great' 'recommended' 'square']\n",
      " ['good' 'with' 'crust' 'pizza' 'here' 'still' 'on' 'weekend' 'general'\n",
      "  'during' 'ny' 'various' 'slice' 'isn' 'spectacular']]\n",
      "good beer brunch flatbread dog patio opted my biscuit gravy calling grilled fry great greatest\n",
      "pizza all we own very on saw than premium one agreed ever mixed ingredient had\n",
      "with fry lunch sub french reasonable nothing free mention presented recently or soda turkey run\n",
      "we pizza had crust with bread delivered cheese ordered only extra into great recommended square\n",
      "good with crust pizza here still on weekend general during ny various slice isn spectacular\n"
     ]
    }
   ],
   "source": [
    "vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "for topic_idx in range(N_TOPIC):\n",
    "    # Get 5 example documents\n",
    "    topic_idx_docs = np.where(dtm_topics == topic_idx)[0][:5]\n",
    "    print(topic_idx_docs)\n",
    "    doc_dtm = dtm[topic_idx_docs].toarray()\n",
    "    print(doc_dtm)\n",
    "    # Get 15 terms from each document\n",
    "    common_terms = np.argsort(doc_dtm, axis=1)[:, ::-1][:,:15]\n",
    "    print(common_terms)\n",
    "    print(vocab_map(common_terms))\n",
    "    document_words = vocab_map(common_terms)\n",
    "    for doc in document_words:\n",
    "        print(\" \".join(doc))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x31595 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 75714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 1000 reviews\n",
    "item_term_matrix, _ = create_matrix(processed_corpus[:1000])\n",
    "item_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 20  6 20  0  8  8 24  1 24 14 24  6  6  2  8 24 19  1 19 19 19 19 23\n",
      " 13  6 17 18 20 20  1  0  3  4 17 15 15 15 15  2 15 17 18  9 13 15  0 15\n",
      " 20  1]\n",
      "[ 4 31 46]\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[  323   332   169 ... 21069 21070 15797]\n",
      " [ 1666   554   190 ... 21069 21070     0]\n",
      " [ 1137   243   612 ... 21069 21070     0]]\n",
      "[[ 323  332  169  338   70]\n",
      " [1666  554  190  171  586]\n",
      " [1137  243  612  169 2057]]\n",
      "[['hot' 'sauce' 'are' 'wing' 'good']\n",
      " ['chinese' 'food' 'even' 'away' 'taste']\n",
      " ['soup' 'best' 'around' 'are' 'chow']]\n",
      "Top topic: 20, Document terms: he, you, him, need, really, have, see, without, parent, referral, patient, on, patronizing, offer, practitioner\n",
      "Top topic: 20, Document terms: he, all, very, been, over, your, dr, patient, year, out, think, health, option, question, one\n",
      "Top topic: 6, Document terms: you, office, he, doctor, not, when, about, away, they, me, call, johnson, will, before, practice\n",
      "Top topic: 20, Document terms: have, dr, doctor, his, very, we, him, many, who, come, been, ha, diagnosed, every, had\n",
      "Top topic: 0, Document terms: hot, sauce, are, wing, good, fish, extra, flavor, frank, red, heat, decent, large, lot, maybe\n",
      "Top topic: 8, Document terms: out, ball, range, are, driving, place, they, you, money, only, sell, light, make, let, mat\n",
      "Top topic: 8, Document terms: look, wait, range, can, on, tee, state, turf, be, yesterday, amazing, nice, ll, really, re\n",
      "Top topic: 24, Document terms: sandwich, were, reuben, fish, on, we, had, they, very, pretty, time, giant, be, extremely, recommendation\n",
      "Top topic: 1, Document terms: you, are, all, place, on, food, re, don, if, good, they, one, get, nthe, lot\n",
      "Top topic: 24, Document terms: good, fish, sandwich, lobsicle, spain, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced\n",
      "Top topic: 14, Document terms: mushroom, an, from, canned, place, they, ever, tasty, know, crowd, were, pizza, staff, bar, night\n",
      "Top topic: 24, Document terms: coleslaw, we, kraut, very, they, charged, be, disappointed, instead, customer, tasting, back, reuben, not, iced\n",
      "Top topic: 6, Document terms: tire, wait, get, they, toyota, there, price, hour, new, good, long, week, lost, make, looked\n",
      "Top topic: 6, Document terms: had, they, tire, called, them, overpriced, seem, summer, super, on, didn, going, also, time, very\n",
      "Top topic: 2, Document terms: lobsicle, sauv, unwanted, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced, prawn, spain\n",
      "Top topic: 8, Document terms: really, don, found, associate, luckily, mill, puppy, teen, treat, on, moved, am, around, young, they\n",
      "Top topic: 24, Document terms: fish, home, good, their, ordered, served, friend, rocky, fry, ate, meatball, brought, he, style, food\n",
      "Top topic: 19, Document terms: eat, definition, sausage, very, fry, here, you, unpretentious, put, thank, country, joint, favorite, home, diner\n",
      "Top topic: 1, Document terms: line, great, ready, stand, people, bad, place, be, meal, never, love, had, portion, good, innovation\n",
      "Top topic: 19, Document terms: you, are, best, place, they, make, healthy, on, grill, breakfast, like, diner, we, many, yourself\n",
      "Top topic: 19, Document terms: they, had, good, made, now, return, toast, what, eat, great, fresh, french, turkey, grill, wait\n",
      "Top topic: 19, Document terms: service, friendly, poached, only, what, eat, crispy, bacon, felt, south, family, frankly, homefries, nfantastic, gab\n",
      "Top topic: 19, Document terms: had, place, nthe, give, really, were, some, decent, price, food, country, which, good, on, service\n",
      "Top topic: 23, Document terms: people, tonya, are, front, very, sweet, desk, helpful, super, prawn, barn, brioni, 5k, measly, influenced\n",
      "Top topic: 13, Document terms: irish, again, bar, great, food, best, not, doe, better, than, place, town, what, nor, service\n",
      "Top topic: 6, Document terms: than, order, phone, his, on, other, he, would, rather, driver, town, just, or, speed, call\n",
      "Top topic: 17, Document terms: were, from, pizza, if, so, ordered, about, there, them, put, no, on, sicilian, bad, meat\n",
      "Top topic: 18, Document terms: you, stuff, are, fried, review, after, really, regret, finish, eating, ljs, cardboard, minute, boat, extra\n",
      "Top topic: 20, Document terms: her, have, had, not, all, them, she, too, tooth, an, because, on, me, care, follow\n",
      "Top topic: 20, Document terms: me, at, review, did, office, door, not, nice, distracted, very, where, there, you, dentist, waited\n",
      "Top topic: 1, Document terms: restaurant, there, favorite, don, food, eat, average, few, myself, carnegie, eating, place, think, service, had\n",
      "Top topic: 0, Document terms: chinese, food, even, away, taste, style, doesn, from, far, real, good, american, ballet, 5k, barn\n",
      "Top topic: 3, Document terms: we, minute, not, meal, came, food, our, an, order, drink, or, so, place, at, waited\n",
      "Top topic: 4, Document terms: cardio, only, you, equipment, gym, room, me, weight, us, training, session, section, personal, join, increase\n",
      "Top topic: 17, Document terms: gym, get, working, egg, try, she, by, if, instead, nasty, most, morning, complain, you, worker\n",
      "Top topic: 15, Document terms: had, time, experience, bread, sandwich, mixed, papa, first, girlfriend, take, ve, server, me, fish, care\n",
      "Top topic: 15, Document terms: white, pizza, bar, order, calamari, go, on, visit, note, nlike, our, ever, ok, style, very\n",
      "Top topic: 15, Document terms: nthe, had, pizza, calamari, white, we, take, time, garlic, very, order, style, some, fried, than\n",
      "Top topic: 15, Document terms: food, everyone, serving, yummy, great, dessert, be, gem, little, place, very, try, without, you, price\n",
      "Top topic: 2, Document terms: liked, place, or, down, great, because, be, california, imagine, relocating, crazy, street, stroll, visitor, couple\n",
      "Top topic: 15, Document terms: restaurant, dish, had, were, on, ordered, made, at, zucchini, not, chicken, marsala, which, she, too\n",
      "Top topic: 17, Document terms: terrible, like, wood, smell, place, wet, rotten, service, food, barn, brioni, lobsicle, prawn, 5k, measly\n",
      "Top topic: 18, Document terms: justify, inconsistent, there, expensive, nice, carnegie, restaurant, price, better, ambiance, are, papa, doesn, spain, innovation\n",
      "Top topic: 9, Document terms: had, we, manager, service, minute, want, you, said, back, get, go, bill, do, our, your\n",
      "Top topic: 13, Document terms: always, place, fresh, so, love, well, food, friendly, staff, delicious, douche, prawn, corresponding, nsuper, caterer\n",
      "Top topic: 15, Document terms: had, back, great, size, on, really, pepper, would, very, what, enough, good, green, just, sauce\n",
      "Top topic: 0, Document terms: soup, best, around, are, chow, salad, some, place, great, bread, love, 5k, innovation, measly, brioni\n",
      "Top topic: 15, Document terms: at, pasta, had, seriously, we, service, went, could, rather, down, hill, price, eat, she, wine\n",
      "Top topic: 20, Document terms: they, have, will, going, nice, be, behind, them, are, stand, re, rude, ve, unprofessional, what\n",
      "Top topic: 1, Document terms: dental, been, you, office, should, time, no, beat, worst, avoid, at, ever, can, any, one\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(item_term_matrix)\n",
    "\n",
    "top_topic = np.argmax(doc_topic, axis=1) #find the topic of the document\n",
    "print(top_topic)\n",
    "\n",
    "for topic_idx in range(N_TOPIC):\n",
    "    # Grab 5 example documents from each topic\n",
    "    topic_idx_docs = np.where(top_topic == topic_idx)[:5]\n",
    "    doc_dtm = item_term_matrix[topic_idx_docs].toarray()\n",
    "    \n",
    "\n",
    "topic_doc_indices = np.where(top_topic == 0)[0][:5] #find documents from topic 0\n",
    "print(topic_doc_indices)\n",
    "print(item_term_matrix[topic_doc_indices].toarray()) #print document term matrix for those documents\n",
    "print(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1]) #argsort to find the most common terms\n",
    "print(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1][:, :5]) #same thing, gives top 5\n",
    "\n",
    "vectorized_vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "print(vectorized_vocab_map(np.argsort(item_term_matrix[topic_doc_indices].toarray(), axis=1)[:,::-1][:, :5])) #get words corresponding to indices\n",
    "\n",
    "for i in range(len(doc_topic)):  # Use the correct range based on your data\n",
    "    # Get the top topic for the document\n",
    "    top_topic = doc_topic[i].argmax()\n",
    "    \n",
    "    # Get the top terms more frequent than 1 in the document from the dtm\n",
    "    row = item_term_matrix[i, :].toarray().flatten()  # Convert sparse row to dense\n",
    "    # item_count = np.count_nonzero(row > 1)\n",
    "    top_terms_indices = row.argsort()[::-1][:15]  # Indices of the top 10 terms\n",
    "    top_terms = [vocab_list[idx] for idx in top_terms_indices]\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Top topic: {top_topic}, Document terms: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try making code to extract 5 documents per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prunes matrices, saves weights but locks model\n",
    "# model.purge_extra_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = f\"model_{N_TOPIC}_{N_ITER}_{RANDOM_STATE}_{REFRESH}_{SEED_CONFIDENCE}.pkl\"\n",
    "\n",
    "# with open(f\"results/{modelname}\", 'wb') as file:\n",
    "    # pkl.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.23365735e-04, 1.55314518e-03, 4.57218647e-04, 2.87699878e-04,\n",
       "        1.87135438e-03, 2.22783475e-04, 2.12591695e-01, 2.84587882e-04,\n",
       "        5.12376868e-02, 5.04084413e-04, 1.43815767e-03, 3.04138392e-04,\n",
       "        3.15682987e-06, 3.29554255e-02, 1.25188044e-02, 3.53338186e-04,\n",
       "        6.78863908e-06, 3.08103614e-04, 4.92289652e-03, 4.52324088e-04,\n",
       "        6.72737138e-01, 2.55559447e-03, 1.49780723e-03, 3.40117465e-04,\n",
       "        2.72587228e-04]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the model can be loaded/works\n",
    "# with open('guidedlda_model.pickle', 'rb') as file:\n",
    "    # model = pkl.load(file)\n",
    "# model.transform(create_matrix(processed_corpus[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-768290932.3669106,\n",
       " -544845646.4478563,\n",
       " -523167504.4543283,\n",
       " -517464505.4454495,\n",
       " -515242153.73157394,\n",
       " -514171247.29473853,\n",
       " -513573261.2712008,\n",
       " -513116435.8723612]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loglikelihoods_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results\n",
    "\n",
    "Make sure to run this after each experiment (after you've created/fitted the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in result_25_500_7_20_0.15.txt\n"
     ]
    }
   ],
   "source": [
    "filename = f\"result_{N_TOPIC}_{N_ITER}_{RANDOM_STATE}_{REFRESH}_{SEED_CONFIDENCE}.txt\"\n",
    "\n",
    "with open(f\"results/{filename}\", \"w\") as f:\n",
    "    if seed_topic_list:\n",
    "        print(seed_topic_list, file=f)\n",
    "    else:\n",
    "        print(\"No Seeds\", file=f)\n",
    "        \n",
    "    dtm_transform = model.transform(dtm)\n",
    "    dtm_topics = np.argmax(dtm_transform, axis=1)\n",
    "    vocab_map = np.vectorize(lambda x: vocab_list[x])\n",
    "\n",
    "\n",
    "    # Save top 25 words per topic\n",
    "    topic_word = model.topic_word_\n",
    "    n_top_words = 25\n",
    "    for idx, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(idx, ' '.join(topic_words)), file=f)\n",
    "        print(\"Example Documents:\", file=f)\n",
    "        \n",
    "        # Get 5 example documents\n",
    "        topic_idx_docs = np.where(dtm_topics == i)[0][:5]\n",
    "        \n",
    "        doc_dtm = dtm[topic_idx_docs].toarray()\n",
    "        \n",
    "        # Get 15 terms from each document\n",
    "        common_terms = np.argsort(doc_dtm, axis=1)[:, ::-1][:,:15]\n",
    "        document_words = vocab_map(common_terms)\n",
    "        for doc in document_words:\n",
    "            print(\" \".join(doc), file=f)\n",
    "        \n",
    "        # Newline as separator\n",
    "        print(file=f)\n",
    "    \n",
    "\n",
    "print(\"Saved in\", filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10701-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
