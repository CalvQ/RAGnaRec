{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pickle as pkl\n",
    "\n",
    "import guidedlda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'goldberg', 'offer', 'everything', 'look', 'for', 'in', 'general', 'practitioner', 'he', 'nice', 'and', 'easy', 'to', 'talk', 'to', 'without', 'being', 'patronizing', 'he', 'always', 'on', 'time', 'in', 'seeing', 'his', 'patient', 'he', 'affiliated', 'with', 'top', 'notch', 'hospital', 'nyu', 'which', 'my', 'parent', 'have', 'explained', 'to', 'me', 'is', 'very', 'important', 'in', 'case', 'something', 'happens', 'and', 'you', 'need', 'surgery', 'and', 'you', 'can', 'get', 'referral', 'to', 'see', 'specialist', 'without', 'having', 'to', 'see', 'him', 'first', 'really', 'what', 'more', 'do', 'you', 'need', 'sitting', 'here', 'trying', 'to', 'think', 'of', 'any', 'complaint', 'have', 'about', 'him', 'but', 'really', 'drawing', 'blank']\n"
     ]
    }
   ],
   "source": [
    "# get the clearned reviews \n",
    "with open(\"clean_reviews.pkl\", \"rb\") as file:\n",
    "    clean_reviews = pkl.load(file)\n",
    "    print(clean_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 2), (56, 3)]\n"
     ]
    }
   ],
   "source": [
    "with open(\"clean_review_corpus.pkl\", \"rb\") as file:\n",
    "    corpus = pkl.load(file)\n",
    "    print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab_list\n",
    "dictionary = Dictionary(clean_reviews)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "vocab_list = list(dictionary.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary.pkl\", \"wb\") as file:\n",
    "    pkl.dump(vocab_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(processed_text, vocabulary=vocab_list):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    dtm = vectorizer.fit_transform(processed_text)\n",
    "    return dtm, vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the seed topics in dict and in matrix\n",
    "# seed_topics = {\n",
    "#     \"doctor\": [\"medicine\", \"office\", \"hurt\"],\n",
    "#     \"food\": [\"coffee\", \"pizza\", \"delicious\"]\n",
    "# }   \n",
    "\n",
    "# processed_seeds = [\" \".join(topic) for topic in list(seed_topics.values())]\n",
    "\n",
    "# seed_topic_list, vocab_dict = create_matrix(processed_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = [\" \".join(tokens) for tokens in clean_reviews]\n",
    "dtm, vocab_dict = create_matrix(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create topic seeds\n",
    "seed_topic_list = [\n",
    "    [\"medicine\", \"office\", \"hurt\"],\n",
    "    [\"coffee\", \"pizza\", \"delicious\"]\n",
    "]\n",
    "\n",
    "seed_topics = {}\n",
    "for topic_id, topic_words in enumerate(seed_topic_list):\n",
    "    for word in topic_words:\n",
    "        seed_topics[vocab_dict[word]] = topic_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "model = guidedlda.GuidedLDA(n_topics=25, n_iter=150, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 700000\n",
      "INFO:guidedlda:vocab_size: 31595\n",
      "INFO:guidedlda:n_words: 65988636\n",
      "INFO:guidedlda:n_topics: 25\n",
      "INFO:guidedlda:n_iter: 150\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "WARNING:guidedlda:all zero column in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -768290932\n",
      "INFO:guidedlda:<20> log likelihood: -544845646\n",
      "INFO:guidedlda:<40> log likelihood: -523167504\n",
      "INFO:guidedlda:<60> log likelihood: -517464505\n",
      "INFO:guidedlda:<80> log likelihood: -515242154\n",
      "INFO:guidedlda:<100> log likelihood: -514171247\n",
      "INFO:guidedlda:<120> log likelihood: -513573261\n",
      "INFO:guidedlda:<140> log likelihood: -513116436\n",
      "INFO:guidedlda:<149> log likelihood: -512954448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x30f16e200>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "# model.fit(dtm, seed_topics=seed_topics, seed_confidence=0.15)\n",
    "model.fit(dtm, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: food good not place they chicken had you have on so like were are very here just be rice thai all sauce restaurant dish at\n",
      "Topic 1: you food place they not have good are service here time if at there be go get so on great just can like been or\n",
      "Topic 2: we food were our had place great service good on time not at very have they there so here drink be back bar out night\n",
      "Topic 3: we were our food had not on they at so there service out table good time be have drink place ordered order came minute back\n",
      "Topic 4: you on at there are have so they not we get room place like all were if be out just up had here your time\n",
      "Topic 5: had were on not good we dish food very so sauce you like they chicken have restaurant which at be all just menu some nthe\n",
      "Topic 6: they me on have not he you had at car be would so them time service when out up an get from back if no\n",
      "Topic 7: we were had our on not at good very so restaurant food which ordered steak they have all be great dinner menu meal service wine\n",
      "Topic 8: you they have are on at not there be like if so or one all your can me store out just get from some show\n",
      "Topic 9: we our were she had at not they he on food so there minute table order out when time be have service me her no\n",
      "Topic 10: they on me not have you we at had so be up out there time when them get no would are he were if from\n",
      "Topic 11: good on you had they not were so have like food pizza at are place buffet just all very be cheese their one we salad\n",
      "Topic 12: taco salsa mexican burrito chip bean tortilla margarita enchilada guacamole carne asada rice food und die da cheese chile der so corn restaurant quesadilla nacho\n",
      "Topic 13: you are have they place great on food good here can there if be at not their or always like get so time ha go\n",
      "Topic 14: you are on place have they there not if at like be bar so can here or good get all great just some your one\n",
      "Topic 15: we had good food were great very not place on pizza service have restaurant so at they you our here be are all which time\n",
      "Topic 16: de le u00e9 et la est un u00e0 pour pa que en une je il du on mais u00e8s au u00e9e qui ce plus ai\n",
      "Topic 17: not food they had place good on so we were have you like just at be pizza time there here ordered out all me if\n",
      "Topic 18: you they are have place food not good on if can so like get their here be at there or go just your time one\n",
      "Topic 19: you they on place have are coffee good so breakfast not had like their at great be there just here one or get all can\n",
      "Topic 20: she me they have not on had her so he at you time be when would back out get very an up there them about\n",
      "Topic 21: we he she our were at not me they had on so there out when have be her no up time one order back just\n",
      "Topic 22: you on at are there place not have if so vega get like they be here room bar all from or can good one out\n",
      "Topic 23: room we at hotel on were you not they had our there have so stay are from no be all out get would one up\n",
      "Topic 24: were had on good not burger we they so food like place have fry just ordered cheese you be very at chicken which all out\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 25\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab_list)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x31595 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1288 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_term_matrix, _ = create_matrix(processed_corpus[:100:5])\n",
    "item_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero column in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topic: 20, Document terms: he, you, him, need, really, have, see, without, parent, referral, patient, on, patronizing, offer, practitioner\n",
      "Top topic: 20, Document terms: have, doctor, many, staff, patient, dr, phone, other, you, answer, get, had, into, incomprehensible, isn\n",
      "Top topic: 8, Document terms: he, all, very, been, over, your, dr, patient, year, out, think, health, option, question, one\n",
      "Top topic: 18, Document terms: be, new, almost, will, position, said, he, there, week, arizona, trying, nyc, doctor, you, think\n",
      "Top topic: 14, Document terms: you, office, he, doctor, not, when, about, away, they, me, call, johnson, will, before, practice\n",
      "Top topic: 9, Document terms: notch, doctor, top, be, him, his, country, one, minimal, nit, referred, manner, he, because, wonderful\n",
      "Top topic: 8, Document terms: have, dr, doctor, his, very, we, him, many, who, come, been, ha, diagnosed, every, had\n",
      "Top topic: 19, Document terms: me, bill, cover, insurance, work, office, visit, can, make, sure, pay, blood, you, physical, an\n",
      "Top topic: 19, Document terms: hot, sauce, are, wing, good, fish, extra, flavor, frank, red, heat, decent, large, lot, maybe\n",
      "Top topic: 9, Document terms: range, are, somewhat, which, golf, tended, city, amenable, amazing, addicted, still, suck, star, defeat, those\n",
      "Top topic: 6, Document terms: out, ball, range, are, driving, place, they, you, money, only, sell, light, make, let, mat\n",
      "Top topic: 21, Document terms: are, place, sell, tee, should, all, on, at, drink, out, they, someone, machine, noticed, order\n",
      "Top topic: 1, Document terms: look, wait, range, can, on, tee, state, turf, be, yesterday, amazing, nice, ll, really, re\n",
      "Top topic: 1, Document terms: service, minute, waiting, pleasant, program, another, visit, old, an, transaction, trade, store, rep, processed, him\n",
      "Top topic: 15, Document terms: sandwich, were, reuben, fish, on, we, had, they, very, pretty, time, giant, be, extremely, recommendation\n",
      "Top topic: 15, Document terms: fish, best, sandwich, stop, miss, can, pittsburgh, measly, 5k, spain, barn, fist, caterer, brioni, nsuper\n",
      "Top topic: 15, Document terms: you, are, all, place, on, food, re, don, if, good, they, one, get, nthe, lot\n",
      "Top topic: 15, Document terms: you, ever, corporate, old, will, traditional, superb, ll, quality, pop, perfection, non, love, fried, great\n",
      "Top topic: 15, Document terms: good, fish, sandwich, lobsicle, spain, rarity, largely, nsuper, caterer, fist, measly, 5k, barn, brioni, influenced\n",
      "Top topic: 24, Document terms: on, fish, had, were, very, good, da, at, another, table, only, we, have, burgh, could\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(item_term_matrix)\n",
    "\n",
    "for i in range(len(doc_topic)):  # Use the correct range based on your data\n",
    "    # Get the top topic for the document\n",
    "    top_topic = doc_topic[i].argmax()\n",
    "    \n",
    "    # Get the top terms more frequent than 1 in the document from the dtm\n",
    "    row = dtm[i, :].toarray().flatten()  # Convert sparse row to dense\n",
    "    item_count = np.count_nonzero(row > 1)\n",
    "    top_terms_indices = row.argsort()[::-1][:15]  # Indices of the top 10 terms\n",
    "    top_terms = [vocab_list[idx] for idx in top_terms_indices]\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"Top topic: {top_topic}, Document terms: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try making code to extract 5 documents per topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10701-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
